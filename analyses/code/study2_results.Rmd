---
title: ""
author: "Jana B. Jarecki"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
documentclass: "apa6"
classoption:
  a4paper,
  man,
  floatsintext
header-includes:
  \usepackage{natbib}
  \usepackage{threeparttable}
  \usepackage{booktabs}
  \shorttitle{test}
  \usepackage{setspace}
  \AtBeginEnvironment{tabular}{\singlespacing}
  \usepackage{times}
  \usepackage{changes}
  \definechangesauthor[name={JJ}, color=orange]{jj}
  \usepackage{upgreek}
  \AtBeginDocument{\let\maketitle\relax}
---

```{r, setup, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
  source("knitr_setup.R")
  source("fig_setup.R")
  library(papaja)
  options(papaja.na_string = "--")
  library(cogscimodels)
  library(data.table)
  library(BayesFactor)
  library(brms)
  library(emmeans)  
  niter <- 1000 # Bayesian modeling iterations
  study <- 2
```

```{r, load_data, cache = TRUE}
  source("tab2.R")
```

```{r, preprocess, cache = TRUE}
  fits <- readRDS("study2_cognitive_models_fit.rds")
  # don't cache this fread, it produces errors
  d <- fread("../../data/processed/study2.csv", colClasses=list(character="id"))
  d <- d[condition=="experience"]
  parameter <- fits[, .(par = names(coef(fit[[1]])), val = coef(fit[[1]])), by = .(id, model)]
  gof <- fits[, as.data.table(cbind(model=c("base","rf","bvu"), anova(fit[[1]], fit[[2]], fit[[3]])[, c("wAIC", "AIC", "BIC")])), by = id]
  weights <- gof[, c("id", "model", "wAIC")]
  weights <- dcast(weights, id ~ model, value.var = "wAIC")
  weights[, winner := names(.SD)[which.max(.SD)], by = id] # winning models
  # Winners
  winners <- sort(table(weights$winner))
  weights[, winner := factor(winner, names(winners), model_labels[names(winners)])]
  N <- gof[, length(unique(id))]
  # Prediction dat
  pred <- fits[, fit[[1]]$predict(), by=.(id,model)]
  pred[, t := 1:.N, by=.(id, model)]
  setnames(pred, "V1", "pred")
  d[, t := 1:.N, by=id]
  dpred <- d[pred, on = c("id", "t")]
  dpred[, pred_scaled := pred]
  dpred[, pred := pred * gamblex]
  dpred <- dpred[weights[, c("id", "winner")], on = "id"]
  dpred[, relfreq_x := gamblep]
  dpred[, count_x := round(gamblep * samplesize)]
  dpred[, count_0 := samplesize - count_x]
  dpred <- dpred[tolower(winner) == model, ]
  dpred <- dpred[parameter[par=="count_x", c("id", "val")], on = "id"]
```


\subsection{Evaluations of Gambles by Condition and Sample Size}
Table \ref{tab:means_study2} displays participants' mean and median evaluations per gamble in the experience and description conditions by sample-size. The results resemble the findings of Study 1, namely that larger sample sizes do not lead to systematic changes in mean evaluations. Evaluations were best predicted by a linear model including expected value as random factor and gamble type as fixed factor gamble (additional fixed factor sample size: $BF\textsubscript{01} = 285$; additional interaction between sample size and gamble type: $BF\textsubscript{01} > 1,000$).

<!-- 
```{r, study2_evaluations, fig.cap = "Mean gamble evaluations of \\$-bets and p-bets in the experience condition across sample sizes and in the the description condition."}
  dagg <- d[, .(M=mean(value), SD=sd(value)), by = .(samplesizecat, gambletype)]
  ggplot(dagg, aes(samplesizecat)) +
    geom_jitter(data=d, aes(y=value), alpha = 0.01) +
    geom_violin(data=d, aes(y=value), color = "grey", fill = "grey", alpha = .4) +
    geom_boxplot(data=d, aes(y=value), fill = NA, color = "white", width = 0.2, size = 1) +
    geom_errorbar(aes(ymin = M-SD, ymax=M+SD), width = 0.1, size = 0.8) +
    geom_point(aes(y=M), size = 3) +
    facet_wrap(~gambletype) +
    ylim(0,27) +
    ylab("Evaluation (M +/- SD)") +
    xlab("Sample Size") +
    labs(caption = "Note: -- denotes description condition") +
    geom_segment(aes(x = "xs", xend = "l", y = 27, yend = 27), color = "grey") +
    geom_label(aes(y = 27, x = "s", label = " Experience"), hjust = 0.03, label.size = 0, size = 2.7, fontface = 3, vjust = 0.4) +
    theme(aspect.ratio = 1)
``` 
-->

```{r, means_study2}
  M <- lapply(M, function(x) cbind(x[, 1:6], replace(round(x[, 7], 0), round(x[, 7], 0) > 1000, ">1000")))
  apa_table(M
    , caption = "Valuations of Gambles in Study 1"
    , col.names = c("Condition", 'Sample size category', 'Effective sample size', '\\textit{Med}', '\\textit{M}', 'D--E', 'D--E:$BF\\textsubscript{10}$')
    , align = c('l', rep('c', 4), 'r', 'r')
    , digits = c(0,0,0,2,2,2,0)
    , note = '\\textit{M} = mean, \\textit{Med} = median, D--E = difference between mean description-based valuations and experience-based valuations, $BF\\textsubscript{10}$ = Bayes Factor quantifying the evidence for a linear model $\\mathrm{M}\\textsubscript{1}$ predicting that valuations differ between description and experience over a linear model $\\mathrm{M}\\textsubscript{0}$ predicting no such differences; both models models contain a by-participant random effect. Gambles IDs 1, 2, and 3 are \\$-bets; Gamble IDs 4, 5, and 6 are p-bets.'
    , escape = FALSE
    )
```

\subsection{Cognitive modeling of experience-based evaluations}
\added[id=jj]{The modeling procedure followed the procedure in Study~1.}

```{r, study2_model_fit}
  N <- nrow(weights)
  bic <- setNames(gof[, round(mean(BIC)), by=model]$V1, c("base", "rf", "bvu"))
```

\subsubsection{Modeling Results}
\added[id=jj]{The next section presents the quantitative cognitive model fit, the qualitative model fit, and lastly the effects of sample size given the cognitive strategies.}

\textit{Quantitative Model Fit.} The Bayesian value updating model described the majority of the participants best (`r winners["bvu"]` of `r N`; `r round(winners["bvu"]/N*100)`\%). The relative frequency model described `r winners["rf"]` participants best (`r round(winners["rf"]/N*100)`\%); the baseline model described no participants best. Figure \ref{fig:study2_model_weights} shows the evidence strength for the models by participant. The models' mean Bayesian information criterion across all participants equaled BIC\textsubscript{BVU}$= `r bic["bvu"]`$, BIC\textsubscript{RF}$= `r bic["rf"]`$, and BIC\textsubscript{BASE}$= `r bic["base"]`$ (lower values indicate better fit).


```{r echo=FALSE}
  knitr::read_chunk("fig_cogn_model_fit.R")
```
```{r, study2_model_weights, echo=FALSE, fig.height = 4, fig.cap = "Evidence for the models for individual participants. \\textit{RF}$=$ relative frequency model, \\textit{BVU}$=$ Bayesian value updating model, \\textit{BASE}$=$ Baseline model."}
  source("fig_cogn_model_fit.R")
  <<cogn_model_fit>>
```

```{r, study2_par}
  parameter <- parameter[weights[, c("id","winner")], on = "id"]
  parameter <- parameter[model == tolower(winner)]
  # Mean parameter to display
  M_alpha_bvu <- parameter[winner=="BVU" & par=="rp", mean(val)]
  M_alpha_rf  <- parameter[winner=="RF" & par=="rp", mean(val)]
  M_thetag_bvu <- parameter[winner=="BVU" & par=="count_x", mean(val)]
  M_theta0_bvu <- parameter[winner=="BVU" & par=="count_0", mean(val)]
  M_delta_bvu <- parameter[winner=="BVU" & par=="delta", mean(val)]
  prior_p_gain = round(M_thetag_bvu / (M_theta0_bvu + M_thetag_bvu) * 100)
  # T-test of alpha parameter (power-utility exponent) called "rp" here
  bf = ttestBF(parameter[winner=="BVU" & par=="rp", val], parameter[winner=="RF" & par=="rp", val])
```

\added[id=jj]{
The estimated parameter of the winning models are shown in Table \ref{tab:study2_parameter}. The mean power utility exponent $\alpha$ is relatively similar for participants classified as using a Bayesian value updating strategy ($M_{\alpha}= `r M_alpha_bvu`$) compared to those using a relative frequency strategy ($M_{\alpha}=`r M_alpha_rf`$), `r apa_print(bf)$full_result`. Like in Study 1, the participants using the Bayesian strategy had a loss prior, on avarage; they believed that gains occur with `r prior_p_gain`\% (gain prior $\theta_G = `r M_thetag_bvu`$; zero-outcome prior $\theta_0 = `r M_theta0_bvu`$). Again, the learning rate parameter $\delta$ shows liberal learning ($M_{\delta}=`r M_delta_bvu`$).
}

```{r, study2_parameter}
  tab <- parameter[, val[grepl(tolower(winner), model)], by=.(winner,par)]
  tab[,
     winner_n := paste0(winner, " (\\textit{n}$=$", ..winners[winner], ")"),
    by=winner]
  tab <- tab[,
    .(M=mean(V1), SD=sd(V1)),
    by = .(winner_n,par)]
  tab[, val := paste0(sprintf("%.2f", M), " (", sprintf("%.2f", SD), ")")]
  tab <- dcast(tab, winner_n ~ par, value.var = "val")
  papaja::apa_table(tab[, c("winner_n", "rp", "delta", "count_x","sigma")]
      , caption = "Parameter Estimates of Winning Models, \\textit{M (SD)}"
      , col.names = c("Winning Model","$\\alpha$", "$\\delta$", "$\\theta_G$","$\\sigma$")
      , align = c("l", rep("c", 4)),
      , note = "\\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model. Parameters denote: $\\alpha=$ power utility exponent, $\\theta_G$ gain prior, $\\sigma$ standard deviation."
      , escape = FALSE
      )
```

```{r, ind_fits_cutoff}
  cutoff <- 0.4
  m_cor <- dpred[tolower(winner) == model, cor(pred, value), by = id][, mean(V1)]
  ids_cutoff <- dpred[tolower(winner) == model, cor(pred, value), by = id][V1 < cutoff]$id
```

\textit{Qualitative Model Fit.}
\added[id=jj]{Figure \ref{fig:ind_fits1} illustrates the qualitative model fit by plotting the predictions of the best-fitting models against the observed evaluations. The models generally describe the data well (mean $r\textsubscript{pred,obs} = `r m_cor`$). However, some participants data are qualitatively not well described by the winning models (participants number `r paste(sort(sprintf("%02.0f", as.numeric(ids_cutoff))), collapse = ", ")`, with $r\textsubscript{pred,obs} < `r cutoff`$). For one of these cases, the relative frequency model was favored, for the remaining three cases the Bayesian value updating model was favored.\footnote{Note, however, that participant number 7 evaluated all gambles with zero, which indicates inattentiveness or not comprehending the task.}}


```{r echo=FALSE}
  knitr::read_chunk("fig_cogn_model_fit_qualitatively.R")
```
```{r, ind_fits1, fig.with = 6, fig.height = 6, fig.cap = "Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model, \\textit{BASE}$=$baseline model."}
  <<cogn_model_fit_qualitatively>>
```

\added[id=jj]{The cognitive modeling results thus show greater heterogeneity in Study 2 (compared to Study 1), slightly more than half the participants could be described by a Bayesian strategy, the remaining participants followed a relative-frequency strategy. The higher prevalence of the relative frequency strategy in Study 2 can be explained considering that Study 2 initially withheld the information about the magnitude of the gain outcomes as opposed to Study 1, where participants received outcome information. If participants learn not only probabilities but also outcomes, the task might require more cognitive effort and this might take attention away from prior beliefs.}

\emph{The effect of sample size given cognitive strategies.} \added[id=jj]{We qualitatively investigated how sample size affects the evaluations of relative-frequency-type and Bayesian-type learners. The Bayesian model predicts that sample size changes the evaluations in interaction with the prior beliefs. Participants with a gain prior should decrease the evaluations of \$-bets as sample sizes increase; participants with a zero-outcome prior should increase their evaluations of p-bets as sample size increases.
}

```{r, study2_bf, cache = TRUE, message=FALSE, results="hide"}
  fit <- readRDS("study2_bayes_models_fit.rds")
  fit_noprior <- readRDS("study2_bayes_models_fit_noprior.rds") # model without the grouping (RF, BVU-gain, BVU-loss)
  BF01 <- brms::bayes_factor(fit, fit_noprior)
  BF01 <- BF01$bf
```

\added[id=jj]{
Given the cognitive modeling results, participants were classified as relative-frequency learners, Bayesian learners with gain priors, and Bayesian learners with loss priors (prior parameters $\theta_G > 1$, and $\theta_G \leq 1$, respectively). Figure \ref{fig:qual2} shows how sample size affects the three learner types' evaluations of p-bets and \$-bets. For \$-bets the evaluation of the Bayesian learners show a small interaction with the prior. Statistical analyses by means of a Bayesian generalized linear model\footnote{regressing the (normalized) evaluations on the predictors sample size, gamble type (p-bet, \$-bet), and type (BVU-gain-prior, BVU-loss-prior, RF) with a by-participant random intercept; categorical predictors were effects-coded to facilitate interpretation of interactions \citep[for details, see][]{SingmannForthcoming}}, however, showed no substantial support that including the learner type as predictor (model M\textsubscript{0}) improves goodness of fit compared to excluding learner type as predictor (model M\textsubscript{0}), BF\textsubscript{01} $=`r BF01`$. These results corroborate the findings from Study 1.
}


```{r echo=FALSE}
  knitr::read_chunk("fig_effect_of_ss_by_model.R")
```
```{r, qual2, fig.cap = "Mean evaluation (standardized to 0 - 1) by winning model and prior beliefs of the BVU model. \\textit{BVU}$=$Bayesian value updating model, \\textit{RF}$=$ Relative frequency model. Error bars indicate standard errors. \\textit{\\$-bet}: low-probability high-outcome gambles, \\textit{p-bet}: high-probability low-outcome gambles. Sample sizes (xs, x, m, l), see Table \\ref{tab:Lotteries}. \\textit{n=16, 13, 6} denotes the number of participants best-described by the respective models."}
  <<effect_of_ss_by_model>>
```




\subsubsection{Effect of gamble type and sampling order}
In line with Study 1, the mean and median ratings in Table \ref{tab:means_study2} show that \$-bets are evaluated higher than p-bets, although he gamble types had the same expected value (BF\textsubscript{10} \textgreater 1,000). Again, we did not find evidence for any recency or primacy effects. $\mathrm{M}\textsubscript{0}$, which predicts valuations as a function of the factor gamble (1--6), was slightly preferred over models that also include the mean of the first half of the observed samples ($\mathrm{M}\textsubscript{1}$) or the mean of the second half of the observed samples ($\mathrm{M}\textsubscript{2}$) as predictors ($BF\textsubscript{01} = 3.4$, $BF\textsubscript{02} = 1.9$). 

\subsubsection{Confidence ratings}
```{r, confidence}
  dpred[, samplesizecat := factor(samplesizecat, levels = c("xs","s","m","l"))]
  MSD <- dpred[condition == "experience",
  .(M=mean(confidence), SD=sd(confidence)), by = .(winner, samplesizecat)]
  MSD <- MSD[order(samplesizecat)]

```
Participants' mean confidence ratings of their valuations from experience in the extra small, small, medium, and large sample-size categories were  ($M (SD)$) equal to 4.01 ($1.23$), 4.09, ($1.14$), 4.04 ($1.19)$, and 4.16 ($1.19$). for sample size categories xs, s, m, and l, respectively. Sample size did not influence participants' confidence systematically: $\mathrm{M}\textsubscript{0}$, which predicts confidence rating as a function of a random participant effect, was strongly preferred over $\mathrm{M}\textsubscript{1}$, which in addition includes the sample-size category as a predictor ($BF\textsubscript{01} = 737$).


```{r, confidence_stats, results="hide"}
# full <- readRDS("study2_bayes_models_fit_confidence.rds")
# null <- readRDS("study2_bayes_models_fit0_confidence.rds")
# BF10 <- bayes_factor(full, null)
BF10 <- 0.01

# full <- readRDS("study2_bayes_models_fit_confidence_rf.rds")
# null <- readRDS("study2_bayes_models_fit0_confidence_rf.rds")
# BF10_rf <- bayes_factor(full, null)
BF10_rf <- 1.43
```

\added[id=jj]{The Bayesian cognitive strategy predicts that confidence increases with higher sample sizes; the relative frequency model predicts that sample size has no influence on confidence. To test this, the confidence data was analyzed separately by best-fitting cognitive model (Bayesian-type learners or relative-frequency-type learners). Contrary to the expectations, the Bayesian learners' confidence did not increase with higher sample sizes ($M=`r MSD[winner=="BVU", M]`$ for sample sizes `r MSD[winner=="BVU", paste(samplesizecat, collapse = ", ")]`, respectively. Respective \textit{SD}s $= `r MSD[winner=="BVU", SD]`$). A linear model\footnote{with by-participant random intercept and the predictors effect-coded.} of the confidence ratings, which excluded sample size as predictor, was preferred over a model including sample size (BF\textsubscript{excl,incl} $= `r 1/BF10`$). Similarly, the confidence of relative-frequency-type learners did not show an effect of sample size on confidence ratings ($M=`r MSD[winner=="RF", M]`$ for sample sizes `r MSD[winner=="RF", samplesizecat]`, respectively; \textit{SD}s $= `r MSD[winner=="RF", SD]`$, respectively; BF\textsubscript{excl,incl} $= `r 1/BF10_rf`$)}.


\subsubsection{Description versus experience}
The above Table \ref{tab:means_study2} summarizes the valuations from description; it also shows the difference between the mean valuations in the experience and description conditions (column D--E, separately for different sample sizes). Further, it provides the Bayes factors, quantifying the evidence in favor of a difference between valuations from experience and those from description.

Valuations made from description and experience differed for most of the gambles and sample sizes (see Table \ref{tab:means_study2}, rightmost column). In particular, participants attached a higher value to experienced than to described \$-bets (Gambles 1--3) but attached a higher value to described than to experienced p-bets (Gambles 4--6). Thus, we found a D--E gap that is the opposite of the classic D--E gap observed in choice paradigms. In our study, participants valued gambles as if they overweighted rare events from description \textit{and} experience. This effect was even stronger when people made valuations from experience.

We also compared participants' confidence ratings of valuations from experience in each sample-size category to those from description ($M = 4.04$, $SD = 1.08$). Separately for each sample-size category, we compared $\mathrm{M}\textsubscript{0}$, which predicts confidence as a function of random participant effects, with $\mathrm{M}\textsubscript{1}$, which takes condition as an additional fixed factor into account. The analyses suggest that participants were slightly more confident about their ratings from experience than from description for small ($BF\textsubscript{10} = 2.5$), medium ($BF\textsubscript{10} = 1.3$), and large ($BF\textsubscript{10} = 4.8$) sample sizes. For the extra small sample sizes ($BF\textsubscript{10} = 0.3$), confidence judgments did not differ. 