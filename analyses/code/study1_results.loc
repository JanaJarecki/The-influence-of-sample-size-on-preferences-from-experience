\ChangesListline {added}{Added\nobreakspace {}(jj)}{relative frequency}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{Bayesian value updating}{1}
\ChangesListline {added}{Added}{The models were compared to a baseline model, predicting a constant evaluation equal to the mean individual evaluation (sensible models are expected to outperform this baseline model).}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The observed and predicted evaluations were normalized to a common range (0 - 1, by division through the gain magnitude and truncation of normalized values $>$ 1). The free model parameters were estimated by maximum likelihood at the participant level, assuming observations follow a truncated normal distribution, censored at 0 and 1, around the model predictions with a constant standard deviation ($\sigma $) estimated as a free parameter ($0 < \sigma \leq 1$). Therefore, the relative frequency model had 2 free parameter, the power utility exponent $\alpha $ ($0 \leq \alpha \leq 20$) and $\sigma $. The Bayesian value updating model had 4 free parameter the gain prior $\theta _G$ ($0 \leq \theta _G \leq 1$; the loss prior was $\theta _0=2-\theta _G$), the learning rate $\delta $ ($0 \leq \delta \leq 10$), $\alpha $ and $\sigma $. A 3-parameter Bayesian value updating model was also tested, which had a fixed learning rate of $\delta =1$ (optimal Bayesian updating). The baseline model had 2 free parameter, the mean evaluation $\mu $ and $\sigma $. We estimated the parameters with a local solver using a augmented Lagrange multiplier method \citep [Rsolnp package, version 1.16]{Ghalanos2015}. We compared the models by the Bayesian information criterion (BIC), transformed into Bayesian evidence weights \citep [][1 means very strong evidence for a competitor model]{Kass1995, Lewandowsky2011}}{3}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.}{3}
\ChangesListline {added}{Added}{To see the qualitative fit between the models and the observed data, we plotted participants' evaluations against the model predictions from the respective participant's best-fitting model with optimized parameters. This allows to examine the qualitative model performance. Figure \ref {fig:ind_fits1} shows that the models generally capture the data well, but in some cases, even the best-fitting model fails to describe the data. Specifically, for participants number 1, 19, 24, and 38, for whom the winning model is the Bayesian updating (BVU) model, even the Bayesian model must be rejected because it mis-fits the data qualitatively.}{4}
\ChangesListline {added}{Added}{Additionally, the cognitive modeling results help interpreting the null finding---that sample size has no effect on evaluations---that we observed at the aggregate level (Table \ref {tab:means_study1}). The aggregate analysis fails to take the heterogeneity in strategy use into account. The distribution of winning model indicates that our sample is best described by a mixture of strategies, which the aggregate analyses could not pick up. Moreover, the aggregate analysis fails to account for the prior beliefs regarding gain probabilities. Depending on the prior belief, the Bayesian value updating (BVU) model predicts either a decrease or an increase in valuations with increasing sample size.}{4}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{ To investigate the interaction of prior beliefs and sample size, we split the participants by cognitive strategy into three groups: relative frequency learners, Bayesian learners with a gain prior (prior parameter $\theta _G > 1$), and Bayesian learners with a loss prior (prior parameter $\theta _G \leq 1$). For relative-frequency learners we do not expect valuations to change with sample size; for the Bayesian learners with a high gain prior we expect valuations to decrease with sample size; for the Bayesian learners with a high zero-outcome prior, we expect valuations to increase with sample size. Figure \ref {fig:qual1} shows---separately for p-bets and \$-bets---how evaluations change for the three groups (relative frequency, Bayesian with gain prior, Bayesian with loss prior). The relative-frequency learners evaluated both p-bets and \$-bets relatively constantly across sample sizes, whereas the Bayesian learners increased their evaluations if they started with a loss prior and decreased their evaluations after starting with a gain prior, at least for p-bets (high-gain low-probability gambles). For \$-bets the Bayesian learners were relatively constant in their evaluations across sample sizes. }{5}
