\ChangesListline {added}{Added\nobreakspace {}(jj)}{relative frequency}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{Bayesian value updating}{1}
\ChangesListline {added}{Added}{The models were compared to a baseline model, which predicts a constant evaluation equal to the mean individual evaluation. Sensible models are expected to outperform this baseline model.}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The observed and predicted evaluations were normalized to a common range (range 0 - 1, by division through the gain magnitude). Maximum likelihood was used to estimate the free model parameters at the participant level, assuming observations follow a truncated normal distribution around the model predictions (truncated between 0 and 1) with a constant standard deviation ($\sigma $), that was estimated as a free parameter ($0 < \sigma \leq 1$). Therefore, the relative frequency model had 2 free parameter, the power utility exponent $\alpha $ ($0 \leq \alpha \leq 20$) and $\sigma $. The Bayesian value updating model had 4 free parameter the gain prior $\theta _G$ ($0 \leq \theta _G \leq 1$), the learning rate $\delta $ ($0 \leq \delta \leq 10$), $\alpha $ and $\sigma $; the loss prior was constrained $\theta _0=2-\theta _G$. The baseline model had 2 free parameter, the mean evaluation $\mu $ and $\sigma $. We estimated the parameters using an augmented Lagrange multiplier method \citep [Rsolnp package, version 1.16]{Ghalanos2015}. We compared the models by the Bayesian information criterion (BIC), transformed into Bayesian evidence weights \citep [][1 means very strong evidence for a competitor model]{Kass1995, Lewandowsky2011}}{3}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.}{3}
\ChangesListline {added}{Added}{The qualitative fit between the models and the data can be examined by plotting observed evaluations against the predicted evaluations, where predictions stem from the respective participant's best-fitting model with optimized parameters. Figure \ref {fig:ind_fits1} shows that the models generally capture the data well, except in four cases, where even the winning model fails to resemble the data. For participants number 1, 19, 24, and 38, for whom the winning model is the Bayesian updating (BVU) model, even the Bayesian model must be rejected because it mis-fits the data qualitatively.}{4}
\ChangesListline {added}{Added}{The heterogeneity in strategies that resulted from the cognitive modeling helps interpreting the null finding---that sample size has no effect on evaluations---that we observed at the aggregate level (Table \ref {tab:means_study1}). The aggregate analysis fails to take the individual differences in strategy use into account. The distribution of winning model indicates that our sample is best described by a mixture of strategies, which the aggregate analyses could not pick up. Moreover, the aggregate analysis fails to account for the prior beliefs regarding gain probabilities. Depending on the prior belief, the Bayesian value updating (BVU) model predicts either a decrease or an increase in valuations with increasing sample size.}{6}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{ Our next analysis examines the interaction between the cognitive strategies and the number of samples that participants drew. Whereas the relative-frequency model holds that higher sample sizes should not affect the evaluation of the gambles, the Bayesian model predicts that sample size changes in the evaluations and that the changes depend on the prior beliefs and the environment. A gain prior---initially believing that gains are more likely than zero-outcomes---should decrease the valuations as sample sizes increase, but only for \$-bets, where gains are in fact less likely (14 to 20\%) than zero-outcomes (Table\nobreakspace {}\ref {table:Lotteries}). By contrast, a zero-outcome prior---initially believing that zero-outcomes are more likely than gains---should increase the valuations with higher sample sizes for p-bets, where in fact gains are more likely (80 to 86\%) than zero-outcomes (Table\nobreakspace {}\ref {table:Lotteries}). }{7}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{ Participants were classified based on the best-fitting model into relative-frequency learners, Bayesian learners with gain priors (parameter $\theta _G > 1$), and Bayesian learners with loss priors (parameter $\theta _G \leq 1$). Figure \ref {fig:qual1} shows the observed change in evaluations with sample size, separately for p-bets and \$-bets. The relative-frequency learners evaluated both p-bets and \$-bets relatively constantly across sample sizes, whereas the Bayesian learners increased their evaluations if they started with a loss prior and decreased their evaluations after starting with a gain prior, at least for p-bets (high-gain low-probability gambles). The Bayesian learners were relatively constant in their evaluations of \$-bets across sample sizes. }{7}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{ A Bayesian generalized linear model of the (normalized) evaluations of gambles as a function of sample size, gamble type (p-bet, \$-bet), and best-fitting cognitive model with a by-participant random intercept confirmed the expectations. Participants that were classified as Bayesian with gain priors decreased their normalized evaluation of \$-bets by about 2\% as sample size increased, trend $-0.020$, 95\% HPD [$-0.034, -0.004$] (where HPD $=$ highest probability density region). Bayesian participants with loss priors did not systematically change their \$-bet evaluations with sample size (trend $-0.000$, 95\% HPD $[-0.011, 0.011]$), and neither did relative-frequency participants (trend $0.011$, 95\% HPD $[-0.004, 0.027]$), and neither did . Regarding p-bets, participants that were best-described by a Bayesian model with loss priors increased their evaluation by about 2\% with increasing sample size, as expected (trend $0.024$, 95\% HPD $[0.014, 0.036]$); participants with gain priors decreased their p-bet evaluations with sample size (trend $-0.003$, 95\% HPD $[-0.017, 0.013]$). Also as expected, the relative-frequency participants did not change their p-bet evaluations with sample size $-0.005$, 95\% HPD $[-0.021, 0.010]$. \par }{7}
