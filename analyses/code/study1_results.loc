\ChangesListline {added}{Added\nobreakspace {}(jj)}{relative frequency}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{Bayesian value updating}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The models were compared to a baseline model, which predicts a constant evaluation equal to the mean individual evaluation. Sensible models are expected to outperform this baseline model.}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The observed and predicted evaluations were normalized to a common range (range 0 - 1, by division through the gain magnitude). Maximum likelihood was used to estimate the free model parameters at the participant level, assuming observations follow a truncated normal distribution around the model predictions (truncated between 0 and 1) with a constant standard deviation ($\sigma $), that was estimated as a free parameter ($0 < \sigma \leq 1$). Therefore, the relative frequency model had 2 free parameter, the power utility exponent $\alpha $ ($0 \leq \alpha \leq 20$) and $\sigma $. The Bayesian value updating model had 4 free parameter the gain prior $\theta _G$ ($0 \leq \theta _G \leq 1$), the learning rate $\delta $ ($0 \leq \delta \leq 10$), $\alpha $ and $\sigma $; the loss prior was constrained $\theta _0=2-\theta _G$. The baseline model had 2 free parameter, the mean evaluation $\mu $ and $\sigma $. We estimated the parameters using an augmented Lagrange multiplier method \citep [Rsolnp package, version 1.16]{Ghalanos2015}. We compared the models by the Bayesian information criterion (BIC), transformed into Bayesian evidence weights \cite {Kass1995, Lewandowsky2011}, with higher weights indicting stronger evidence favoring a model.}{3}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.}{3}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The qualitative fit between the models and the data is shown in Figure \ref {fig:ind_fits1}, which plots the predictions of the best-fitting models against the observed evaluations. It shows that the models generally describe the data well (mean $r\textsubscript {pred,obs} = 0.71$), except in four cases, where even the winning model fails to resemble the data qualitatively (participants number 19, 24, 38, 5, with $r\textsubscript {pred,obs} < 0.40$). For these cases, for whom the winning model is the Bayesian updating model, the model must be rejected because of qualitative mis-fit.}{4}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The cognitive modeling results thus show that most participants used a Bayesian strategy, and some used a relative-frequency strategy. This strategy heterogeneity helps understanding the behavioral null finding---that sample size seemed to have no effect on valuations---that were observed at the aggregate level (Table \ref {tab:means_study1}). The aggregate analysis fails to take the individual differences in learning strategies into account, while participants are best described by a mixture of strategies. Moreover, the aggregate analysis also fails to account for differences in the prior beliefs about gain probabilities. Depending on the prior belief, the Bayesian value updating (BVU) model predicts either a decrease or an increase in valuations with increasing sample size. The next analysis will focus on these differences.}{6}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{ The Bayesian model predicts that sample size changes the evaluations differently as a function of prior beliefs. Participants with a gain prior---initially believing that gains are more likely than zero-outcomes---should decrease the evaluations of \$-bets as sample sizes increase, because participants overwrite their priors through sampling and learn that gains of \$-bets are less likely than zero-outcomes. By contrast, participants with a zero-outcome prior---initially believing that zero-outcomes are more likely than gains---should increase their evaluations of p-bets as sample size increases, because they learn that gains of p-bets are more likely than zero-outcomes (see the probabilities in Table\nobreakspace {}\ref {table:Lotteries}). }{6}
