\ChangesListline {added}{Added\nobreakspace {}(jj)}{relative frequency}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{Bayesian value updating}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The models were compared to a baseline model, which predicts a constant evaluation equal to the mean individual evaluation. Sensible models are expected to outperform this baseline model.}{1}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The observed and predicted evaluations were normalized to a common range (range 0 - 1, by division through the gain magnitude). Maximum likelihood was used to estimate the free model parameters at the participant level, assuming observations follow a truncated normal distribution around the model predictions (truncated between 0 and 1) with a constant standard deviation ($\sigma $), that was estimated as a free parameter ($0 < \sigma \leq 1$). Therefore, the relative frequency model had 2 free parameter, the power utility exponent $\alpha $ ($0 \leq \alpha \leq 20$) and $\sigma $. The Bayesian value updating model had 4 free parameter the gain prior $\theta _G$ ($0 \leq \theta _G \leq 1$), the learning rate $\delta $ ($0 \leq \delta \leq 10$), $\alpha $ and $\sigma $; the loss prior was constrained $\theta _0=2-\theta _G$. The baseline model had 2 free parameter, the mean evaluation $\mu $ and $\sigma $. We estimated the parameters using an augmented Lagrange multiplier method \citep [Rsolnp package, version 1.16]{Ghalanos2015}. We compared the models by their evidence strength and BIC weights on the Bayesian information criterion (BIC) \cite [evidence in favor of a model compared to the individually best-fitting model][]{Kass1995, Lewandowsky2011}. Higher weights indicate stronger evidence for a model.}{3}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.}{3}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{ The estimated parameter of the winning models, which Table \ref {tab:study1_parameter} summarizes, reveal that the power utility exponent ($\alpha $) is almost identical for the participants using a Bayesian value updating strategy ($M_{\alpha }= 1.43$) and those using a relative frequency strategy ($M_{\alpha }=1.60$), $M = -0.13$ 95\% HDI $[-0.71$, $0.38]$, $\mathrm {BF}_{\textrm {01}} = 3.26$. Participants using the Bayesian strategy had, on average, a prior belief that gains occur with 46\% (gain prior $\theta _G = 0.92$; zero-outcome prior $\theta _0 = 1.08$). Also, their estimated learning rate $\delta $ was anti-conservative ($M_{\delta }=1.36$; values $>$ 1 are liberal, 1 is optimal Bayesian, $<$ 1 is conservative learning); this contradicts previous results that found conservative learners \citep {Edwards1967,Tauber2017}, but the liberal learning in our task can be explained because participants repeatedly sampled from the same set of gambles. }{4}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The qualitative fit between the models and the data is shown in Figure \ref {fig:ind_fits1}, which plots the predictions of the best-fitting models against the observed evaluations. It shows that the models generally describe the data well (mean $r\textsubscript {pred,obs} = 0.71$), except in four cases, where even the winning model fails to resemble the data qualitatively (participants number 05, 19, 24, 38, with $r\textsubscript {pred,obs} < 0.40$). For these cases, for whom the winning model is the Bayesian updating model, the model must be rejected because of qualitative mis-fit.}{4}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{The cognitive modeling results thus show that most participants used a Bayesian strategy, and some used a relative-frequency strategy. This strategy heterogeneity helps understanding the behavioral null finding---that sample size seemed to have no effect on valuations---that were observed at the aggregate level (Table \ref {tab:means_study1}). The aggregate analysis fails to take the individual differences in learning strategies into account, while participants are best described by a mixture of strategies. Moreover, the aggregate analysis also fails to account for differences in the prior beliefs about gain probabilities. Depending on the prior belief, the Bayesian value updating (BVU) model predicts either a decrease or an increase in valuations with increasing sample size. The next analysis will focus on these differences.}{4}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{ The Bayesian model predicts that sample size changes the evaluations differently as a function of prior beliefs. Participants with a gain prior---initially believing that gains are more likely than zero-outcomes---should decrease the evaluations of \$-bets as sample sizes increase, because participants overwrite their priors through sampling and learn that gains of \$-bets are less likely than zero-outcomes. By contrast, participants with a zero-outcome prior---initially believing that zero-outcomes are more likely than gains---should increase their evaluations of p-bets as sample size increases, because they learn that gains of p-bets are more likely than zero-outcomes (see the probabilities in Table\nobreakspace {}\ref {table:Lotteries}). }{5}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{that includes the expected value as random effect}{7}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{that includes p-bets and \$-bets as fixed effects}{7}
\ChangesListline {added}{Added\nobreakspace {}(jj)}{According to Bayesian value updating higher sample sizes should increase confidence. The relative frequency model predicts no influence of sample size on confidence. We analyzed the confidence ratings by best-fitting model (Bayesian-type learners and relative-frequency-type learners). The confidence of Bayesian learners did not change remarkably across sample sizes ($M=4.17, 4.09, 4.18, 4.17$ for sample sizes s, xs, l, m, respectively; $SD$s$=$ 0.99, 1.09, 1.02, 1.02, respectively). A linear model\rmarkdownfootnote {with by-participant random intercept and the predictors effect-coded.} of the confidence ratings, which excluded sample size as predictor, was preferred over a model including sample size ($BF\textsubscript {excl,incl} = 3.01$). Similarly, the confidence of relative-frequency-type learners did not show an effect of sample size on confidence ratings ($M = 4.07, 4.14, 4.10, 4.10$ for sample sizes mxssl, respectively; \textit {SD}s$=$ 1.08, 1.17, 1.15, 1.20, respectively; BF\textsubscript {excl,incl}$= 0.00348182551069965FALSE$)}{8}
