---
title: ""
author: "Jana B. Jarecki"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
documentclass: "apa6"
classoption:
  a4paper,
  man,
  floatsintext
header-includes:
  \usepackage{natbib}
  \usepackage{threeparttable}
  \usepackage{booktabs}
  \shorttitle{test}
  \usepackage{setspace}
  \AtBeginEnvironment{tabular}{\singlespacing}
  \usepackage{times}
  \usepackage{changes}
  \usepackage{upgreek}
  \AtBeginDocument{\let\maketitle\relax}
---

```{r, setup, echo=FALSE, warning=FALSE, message=FALSE, results="hide"}
knitr::opts_chunk$set(
  comment = NA,
  results = "asis",
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  fig.path = "../figures/",
  knitr.kable.NA = "",
  dev = "cairo_pdf",
  fig.align = "center")
knitr::opts_knit$set(
  base.url = "/",
  base.dir = "../../")

library(papaja)
library(cogscimodels)
library(data.table)
source("fig_setup.R")
```

\subsection{Evaluations of Gambles by Condition and Sample Size}
Table \ref{tab:meansStudy1} shows participants' evaluations of the gambles in the experience and description conditions. The evaluations in the experience condition are not a linear function of sample size. In the best mixed regression model $\mathrm{M}\textsubscript{0}$ gamble type (p-bet vs. \$-bet) has a (fixed) effect on evaluations and sample size is only a random factor. It outperforms a model with sample size as fixed effect ($BF\textsubscript{01} = 409$) and a model with sample-size$\times$gamble-type interaction ($BF\textsubscript{01} > 1,000$).

```{r, include=0}
source('tab2.R')
```

```{r, study1_evaluations, fig.cap = "Mean gamble evaluations of \\$-bets and p-bets in the experience condition across sample sizes and in the the description condition."}
dagg <- d[, .(M=mean(value), SD=sd(value)), by = .(samplesizecat, gambletype)]
ggplot(dagg, aes(samplesizecat)) +
  geom_jitter(data=d, aes(y=value), alpha = 0.01) +
  geom_violin(data=d, aes(y=value), color = "grey", fill = "grey", alpha = .4) +
  geom_boxplot(data=d, aes(y=value), fill = NA, color = "white", width = 0.2, size = 1) +
  geom_errorbar(aes(ymin = M-SD, ymax=M+SD), width = 0.1, size = 0.8) +
  geom_point(aes(y=M), size = 3) +
  facet_wrap(~gambletype) +
  ylim(0,27) +
  ylab("Evaluation (M +/- SD)") +
  xlab("Sample Size") +
  labs(caption = "Note: -- denotes description condition") +
  geom_segment(aes(x = "xs", xend = "l", y = 27, yend = 27), color = "grey") +
  geom_label(aes(y = 27, x = "s", label = " Experience"), hjust = 0.03, label.size = 0, size = 2.7, fontface = 3, vjust = 0.4) +
  theme(aspect.ratio = 1)
```


```{r, meansStudy1}
options(knitr.kable.NA = '')
apa_table(M
    , caption = 'Valuations of Gambles in Study 1'
    , col.names = c('Condition', 'Sample size', '\\textit{Med}', '\\textit{M}', 'D--E', 'D--E:$BF\\textsubscript{10}$')
    , align = c('l', rep('c', 3), 'r', 'r'),
    , note = '\\textit{M} = mean, \\textit{Med} = median, D--E = difference between mean description-based valuations and experience-based valuations, $BF\\textsubscript{10}$ = Bayes Factor quantifying the evidence for a linear model $\\mathrm{M}\\textsubscript{1}$ predicting that valuations differ between description and experience over a linear model $\\mathrm{M}\\textsubscript{0}$ predicting no such differences; both models models contain a by-participant random effect. Gambles IDs 1, 2, and 3 are \\$-bets; Gamble IDs 4, 5, and 6 are p-bets.'
    , escape = F
    )
```

\subsection{Cognitive Modeling of Evaluations by Sample Size}
To examine the role of sample size in value judgments more closely, we used cognitive computational modeling. We compared the performance of the \added{relative frequency} (RF) model and the \added{Bayesian value updating} (BVU) model regarding the observed evaluations. \added{The models were compared to a baseline model, predicting a constant evaluation equal to the mean  individual evaluation (sensible models are expected to outperform this baseline model).}

\subsubsection{Modeling Procedure} 
\added{The observed and predicted evaluations were normalized to a common range (0 - 1, by division through the gain magnitude and truncation of normalized values $>$ 1). The free model parameters were estimated by maximum likelihood at the participant level, assuming observations follow a truncated normal distribution (truncation: 0-1) around the model predictions with a constant standard deviation ($\sigma$) estimated as a free parameter ($0 < \sigma \leq 1$).  Therefore, the RF model had 2 free parameter, the power utility exponent $\alpha$ ($0 \leq \alpha \leq 20$) and $\sigma$. The BVU model had 4 free parameter the gain prior $\theta_G$ ($0 \leq \theta_G \leq 1$; the loss prior was $\theta_0=2-\theta_G$), the learning rate $\delta$ ($0 \leq \delta \leq 10$), $\alpha$ and $\sigma$. A 3-parameter BVU model was also tested, constraining the BVU to have a fixed learning rate of $\delta=1$ (optimal Bayesian updating). The baseline model had 2 free parameter, the mean evaluation $\mu$ and $\sigma$. Parameters were estimated by a local nonlinear optimization solver solnp using the R package solnp (REF). The model comparison used the Bayesian information criterion (BIC), transformed into Bayesian evidence weights (\citealp{Kass1995, Lewandowsky2011}).}

```{r, model_fit, echo=FALSE, warning=FALSE}
indiv_models <- readRDS("../fitted_models/study1_fitted_models.Rds")
weights <- lapply(indiv_models, function(id) setNames(as.list(akaike_weight(sapply(id, function(m) m$BIC()))), names(id)))
weights <- rbindlist(weights, id = "id")
weights[, winner := names(.SD)[which.max(.SD)], by = id]
weights[, winner := factor(winner, levels = model_levels)]
winners <- sort(table(weights$winner))
winners <- winners[c(2:4,1)]
weights[, winner := factor(winner, levels = names(winners), labels = model_labels[names(winners)])]

n_bvu <- winners["bvu"]
n_bvu2 <- winners["bvu3"]
n_rf <- winners["rf"]
n_base <- winners["baseline"]
N <- nrow(weights)

bic <- rbindlist(lapply(indiv_models, function(id) lapply(id, function(m) m$BIC())), id = "id")
bic <- rbindlist(lapply(indiv_models, function(id) lapply(id, function(m) m$logLik())), id = "id")
bic <- round(unlist(bic[, lapply(.SD, median), .SDcols = 2:5]), 1)
```



\subsubsection{Modeling Results}
The Bayesian value updating model described most participants best (`r n_bvu` of `r N`; `r round(n_bvu/N*100)`\%), and the relative frequency model described a minority of participants   (`r n_rf`; `r round(n_rf/N*100)`\%). The baseline model described `r n_base` participants best. The 3-parameter Bayesian value updating model (BVU $\delta=1$) described no participant best. Figure \ref{fig:model_weights} shows the evidence strength for the models by participant. It becomes evident that the 3-parameter BVU competes mostly with the full BVU model. The median (across participant) Bayesian information criterion (BIC) of the models was BIC\textsubscript{BVU}$=$ `r bic["bvu"]` BIC\textsubscript{BCU $\delta=1$} $=$ `r bic["bvu3"]`, BIC\textsubscript{RF}$=$ `r bic["rf"]`.





```{r, model_weights, fig.width = 5, fig.height = 4 , fig.cap = "Evidence for the models for individual participants. RF: relative frequency model, BVU: Bayesian value updating model, BASE: Baseline model."}
ids <- weights$id
grouped_weights <- weights[, c("bvu", "baseline", "rf")] + cbind(weights[, "bvu3"], 0 , 0)
id_order <- order_evidence(weights$id, grouped_weights)
weights2 <- melt(weights, 1, c(2:5))
weights2[, variable := factor(variable, labels = model_labels, levels = model_levels)]

 plot1 <- ggplot(weights, aes(x=1, fill=winner)) +
  geom_bar(position = "stack") +
  scale_fill_manual("Model",
    values = model_colors,
    labels = unname(model_labels_bquote[names(winners)]),
    drop = FALSE,
    guide = guide_legend(reverse = TRUE)) +
  scale_y_continuous(expand = c(0,0.15)) +
  xlab("Count")+
  geom_text(stat = "count", aes(label = (..count..), color = winner), position = position_stack(vjust = 0.5), family = "Arial", size = 3.4) +
  scale_color_manual(values = c("black", "black", "white")) +
  coord_flip() +
  guides(color = "none") +
  theme(
    axis.line = element_blank(),
    axis.ticks = element_blank(),
    axis.title.x = element_blank(),
    axis.text = element_blank(),
    legend.spacing.x = unit(0.4, "lines"),
    legend.text = element_text(margin = margin(r = 0.4, unit = "lines")),
    legend.key = element_rect(color = "grey", size = 0.4))

plot2 <- ggplot(weights2,
    aes(x = factor(id, levels = id_order),  y = value, fill = variable)) +
  geom_bar(stat = "identity", color = "white", size = 0.01) +
  geom_hline(yintercept = 0.5, linetype = 2, color = "grey80", alpha = 0.8, size = 0.4) +
  scale_fill_manual("Model", values = model_colors, labels = unname(model_labels_bquote)) +
  scale_y_continuous("Evidence Strength", expand  = c(0,0), labels = percent) +
  xlab("Participant") +
  guides(fill = guide_legend(override.aes = list(linetype = 0, size = 4.5))) +
  theme(axis.ticks = element_blank(), 
    axis.text.x=element_blank(),
    legend.position = "none",
    legend.direction = "vertical",
    axis.line = element_blank(),
    axis.text.y = element_text(vjust = c(0,.5,.5,.5,1)),
    axis.title.x = element_text(margin = margin(t = 0.5, unit = "lines")),
    legend.key.height = unit(1.2, "lines"),
    legend.key = element_rect(fill = "white")) 

plot1 + plot2 +plot_layout(nrow=2, ncol = 1, heights = c(.15,.8))
```

```{r, parameter}
parameter <- lapply(indiv_models, function(id) as.list(unlist(setNames(lapply(id, function(m) m$coef()), names(id)))))
parameter <- rbindlist(parameter, id = "id")
parameter <- parameter[weights[, c("id","winner")], on = "id"]
```

The estimated parameter of the winning models (Table~\ref{tab:model_par}) reveal no apparent difference between the utility (power utility exponent $\alpha$) between those participants using the Bayesian value updating ($M=1.68$) and those using a relative frequency strategy ($M=1.73$) `r apa_print(t.test(parameter[winner=="BVU", bvu.rp], parameter[winner=="RF", rf.rp]))$full_result`. The resulting parameters of the Bayesian model also indicate a prior belief in zero outcomes being more likely (62\%) than a gain ($\theta_G = 0.77, \theta_0 = 1.23$ prior on gains and zeros, respectively). Interestingly, the estimated learning rate ($\delta$) was anti-conservative (values $<$ 1 mean conservative, 1 is optimal Bayesian, $>$ 1 is liberal learning), unlike usually conservative Bayesian learners \citep{Edwards1967,Tauber2017}).


```{r, model_par}
tab <- parameter[, lapply(.SD, function(i) paste0(sprintf("%.2f", mean(i)), " (", sprintf("%.2f", sd(i)), ")")), .SDcols = 2:14, by = winner]
tab <- tab[, replace(.SD, !grepl(tolower(winner), tolower(names(.SD))), NA_character_), by = winner]
tab <- tab[, c(1:3,5:6,13:14,11:12)]
tab[, sigma := na.omit(c(bvu.sigma, rf.sigma, baseline.sigma))]
tab[, alpha := na.omit(c(bvu.rp, rf.rp, "-"))]
tab <- replace(tab, is.na(tab), "-")
tab[, winner := paste0(winner, " (\\textit{n}$=$", winners[c("bvu","rf","baseline")], ")")]

apa_table(tab[, c("winner","alpha","bvu.delta","bvu.count_x","baseline.m","sigma")]
    , caption = "Parameter Estimates of Winning Models (\\textit{M (SD)})"
    , col.names = c("Winning Model","$\\alpha$","$\\delta$","$\\theta_G$","$\\mu$","$\\sigma$")
    , align = c("l", rep("c", 5)),
    , note = "BVU: Bayesian value updating model, RF: relative frequency model, BASE: baseline model. Parameters denote: $\\alpha=$ power utility exponent, $\\delta=$ learning rate, $\\theta_G$ gain prior, $\\mu=$ mean evaluation, $\\sigma$ standard deviation."
    , escape = FALSE
    )

```