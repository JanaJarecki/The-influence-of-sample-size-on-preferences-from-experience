\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Ghalanos2015}
\citation{Kass1995,Lewandowsky2011}
\@writefile{toc}{\contentsline {subsection}{Evaluations of Gambles by Condition and Sample Size}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Cognitive Modeling of Experience-based Evaluations by Sample Size}{1}{section*.4}\protected@file@percent }
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{relative frequency}{1}}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{Bayesian value updating}{1}}
\@writefile{loc}{\ChangesListline {added}{Added}{The models were compared to a baseline model, predicting a constant evaluation equal to the mean individual evaluation (sensible models are expected to outperform this baseline model).}{1}}
\@writefile{toc}{\contentsline {subsubsection}{Modeling Procedure}{1}{section*.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Valuations of Gambles in Study 1\relax }}{2}{table.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:means_study1}{{1}{2}{Valuations of Gambles in Study 1\relax }{table.caption.3}{}}
\citation{Edwards1967,Tauber2017}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{The observed and predicted evaluations were normalized to a common range (0 - 1, by division through the gain magnitude and truncation of normalized values $>$ 1). The free model parameters were estimated by maximum likelihood at the participant level, assuming observations follow a truncated normal distribution, censored at 0 and 1, around the model predictions with a constant standard deviation ($\sigma $) estimated as a free parameter ($0 < \sigma \leq 1$). Therefore, the relative frequency model had 2 free parameter, the power utility exponent $\alpha $ ($0 \leq \alpha \leq 20$) and $\sigma $. The Bayesian value updating model had 4 free parameter the gain prior $\theta _G$ ($0 \leq \theta _G \leq 1$; the loss prior was $\theta _0=2-\theta _G$), the learning rate $\delta $ ($0 \leq \delta \leq 10$), $\alpha $ and $\sigma $. A 3-parameter Bayesian value updating model was also tested, which had a fixed learning rate of $\delta =1$ (optimal Bayesian updating). The baseline model had 2 free parameter, the mean evaluation $\mu $ and $\sigma $. We estimated the parameters with a local solver using a augmented Lagrange multiplier method \citep  [Rsolnp package, version 1.16]{Ghalanos2015}. We compared the models by the Bayesian information criterion (BIC), transformed into Bayesian evidence weights \citep  [][1 means very strong evidence for a competitor model]{Kass1995, Lewandowsky2011}}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Modeling Results}{3}{section*.6}\protected@file@percent }
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Quantitative Model Fit.}{3}{section*.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Evidence for the models for individual participants. \textit  {RF}$=$ relative frequency model, \textit  {BVU}$=$ Bayesian value updating model, \textit  {BASE}$=$ Baseline model.\relax }}{3}{figure.caption.8}\protected@file@percent }
\newlabel{fig:model_weights}{{1}{3}{Evidence for the models for individual participants. \textit {RF}$=$ relative frequency model, \textit {BVU}$=$ Bayesian value updating model, \textit {BASE}$=$ Baseline model.\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parameter Estimates of Winning Models, \textit  {M (SD)}\relax }}{4}{table.caption.10}\protected@file@percent }
\newlabel{tab:model_par}{{2}{4}{Parameter Estimates of Winning Models, \textit {M (SD)}\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Qualitative Model Fit.}{4}{section*.11}\protected@file@percent }
\@writefile{loc}{\ChangesListline {added}{Added}{The qualitative fit between the models and the data can be examined by plotting observed evaluations against the predicted evaluations, where predictions stem from the respective participant's best-fitting model with optimized parameters. Figure \ref  {fig:ind_fits1} shows that the models generally capture the data well, except in four cases, where even the winning model fails to resemble the data. For participants number 1, 19, 24, and 38, for whom the winning model is the Bayesian updating (BVU) model, even the Bayesian model must be rejected because it mis-fits the data qualitatively.}{4}}
\@writefile{loc}{\ChangesListline {added}{Added}{The heterogeneity in strategies that resulted from the cognitive modeling helps interpreting the null finding---that sample size has no effect on evaluations---that we observed at the aggregate level (Table \ref  {tab:means_study1}). The aggregate analysis fails to take the individual differences in strategy use into account. The distribution of winning model indicates that our sample is best described by a mixture of strategies, which the aggregate analyses could not pick up. Moreover, the aggregate analysis fails to account for the prior beliefs regarding gain probabilities. Depending on the prior belief, the Bayesian value updating (BVU) model predicts either a decrease or an increase in valuations with increasing sample size.}{4}}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{ Our next analysis examines the interaction between the cognitive strategies and the number of samples that participants drew. Whereas the relative-frequency model holds that higher sample sizes should not affect the evaluation of the gambles, the Bayesian model predicts that sample size changes in the evaluations and that the changes depend on the prior beliefs and the environment. A gain prior---initially believing that gains are more likely than zero-outcomes---should decrease the valuations as sample sizes increase, but only for \$-bets, where gains are in fact less likely (14 to 20\%) than zero-outcomes (Table\nobreakspace  {}\ref  {table:Lotteries}). By contrast, a zero-outcome prior---initially believing that zero-outcomes are more likely than gains---should increase the valuations with higher sample sizes for p-bets, where in fact gains are more likely (80 to 86\%) than zero-outcomes (Table\nobreakspace  {}\ref  {table:Lotteries}). }{5}}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{ Participants were classified based on the best-fitting model into relative-frequency learners, Bayesian learners with gain priors (parameter $\theta _G > 1$), and Bayesian learners with loss priors (parameter $\theta _G \leq 1$). Figure \ref  {fig:qual1} shows the observed change in evaluations with sample size, separately for p-bets and \$-bets. The relative-frequency learners evaluated both p-bets and \$-bets relatively constantly across sample sizes, whereas the Bayesian learners increased their evaluations if they started with a loss prior and decreased their evaluations after starting with a gain prior, at least for p-bets (high-gain low-probability gambles). The Bayesian learners were relatively constant in their evaluations of \$-bets across sample sizes. }{5}}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{ A Bayesian generalized linear model of the (normalized) evaluations of gambles as a function of sample size, gamble type (p-bet, \$-bet), and best-fitting cognitive model with a by-participant random intercept confirmed the expectations. Participants classified as Bayesian with gain priors decreased their evaluation of \$-bets by about 2\% as sample size increased, trend $-0.020$, 95\% HPD [$-0.034, -0.004$]. Also as expected, participants that were best-described by relative-frequency strategy did not change their \$-bet evaluations with sample size (trend $0.011$, 95\% HPD $[-0.004, 0.027]$), and neither did Bayesian participants with loss priors (trend $-0.000$, 95\% HPD $[-0.011, 0.011]$). Regarding p-bets, participants that were best-described by a Bayesian model with loss priors increased their evaluation by about 2\% with increasing sample size, as expected (trend $0.024$, 95\% HPD $[0.014, 0.036]$); participants with gain priors decreased their p-bet evaluations with sample size (trend $-0.003$, 95\% HPD $[-0.017, 0.013]$). Also as expected, the relative-frequency participants did not change their p-bet evaluations with sample size $-0.005$, 95\% HPD $[-0.021, 0.010]$. \par }{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \textit  {BVU}$=$ Bayesian value updating model, \textit  {RF}$=$ relative frequency model, \textit  {BASE}$=$baseline model.\relax }}{7}{figure.caption.12}\protected@file@percent }
\newlabel{fig:ind_fits1}{{2}{7}{Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \textit {BVU}$=$ Bayesian value updating model, \textit {RF}$=$ relative frequency model, \textit {BASE}$=$baseline model.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean evaluation (standardized to 0 - 1) split by winning model and prior beliefs of the BVU model. \textit  {BVU}$=$Bayesian value updating model, \textit  {RF}$=$ Relative frequency model. Error bars indicate standard errors. \textit  {\$-bet}: low-probability high-outcome gambles, \textit  {p-bet}: high-probability low-outcome gambles. Sample sizes (xs, x, m, l), see Table \ref  {tab:Lotteries}. \textit  {n=16, 13, 6} denotes the number of participants best-described by the respective models.\relax }}{8}{figure.caption.13}\protected@file@percent }
\newlabel{fig:qual1}{{3}{8}{Mean evaluation (standardized to 0 - 1) split by winning model and prior beliefs of the BVU model. \textit {BVU}$=$Bayesian value updating model, \textit {RF}$=$ Relative frequency model. Error bars indicate standard errors. \textit {\$-bet}: low-probability high-outcome gambles, \textit {p-bet}: high-probability low-outcome gambles. Sample sizes (xs, x, m, l), see Table \ref {tab:Lotteries}. \textit {n=16, 13, 6} denotes the number of participants best-described by the respective models.\relax }{figure.caption.13}{}}
