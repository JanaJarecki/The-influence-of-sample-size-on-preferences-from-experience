\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Ghalanos2015}
\citation{Kass1995,Lewandowsky2011}
\@writefile{toc}{\contentsline {subsection}{Evaluations of Gambles by Condition and Sample Size}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Cognitive modeling of experience-based evaluations}{1}{section*.4}\protected@file@percent }
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{relative frequency}{1}}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{Bayesian value updating}{1}}
\@writefile{loc}{\ChangesListline {added}{Added}{The models were compared to a baseline model, which predicts a constant evaluation equal to the mean individual evaluation. Sensible models are expected to outperform this baseline model.}{1}}
\@writefile{toc}{\contentsline {subsubsection}{Modeling Procedure}{1}{section*.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Valuations of Gambles in Study 1\relax }}{2}{table.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:means_study1}{{1}{2}{Valuations of Gambles in Study 1\relax }{table.caption.3}{}}
\citation{Edwards1967,Tauber2017}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{The observed and predicted evaluations were normalized to a common range (range 0 - 1, by division through the gain magnitude). Maximum likelihood was used to estimate the free model parameters at the participant level, assuming observations follow a truncated normal distribution around the model predictions (truncated between 0 and 1) with a constant standard deviation ($\sigma $), that was estimated as a free parameter ($0 < \sigma \leq 1$). Therefore, the relative frequency model had 2 free parameter, the power utility exponent $\alpha $ ($0 \leq \alpha \leq 20$) and $\sigma $. The Bayesian value updating model had 4 free parameter the gain prior $\theta _G$ ($0 \leq \theta _G \leq 1$), the learning rate $\delta $ ($0 \leq \delta \leq 10$), $\alpha $ and $\sigma $; the loss prior was constrained $\theta _0=2-\theta _G$. The baseline model had 2 free parameter, the mean evaluation $\mu $ and $\sigma $. We estimated the parameters using an augmented Lagrange multiplier method \citep  [Rsolnp package, version 1.16]{Ghalanos2015}. We compared the models by the Bayesian information criterion (BIC), transformed into Bayesian evidence weights \cite  {Kass1995, Lewandowsky2011}, with higher weights indicting stronger evidence favoring a model.}{3}}
\@writefile{toc}{\contentsline {subsubsection}{Modeling Results}{3}{section*.6}\protected@file@percent }
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Evidence for the models for individual participants. \textit  {RF}$=$ relative frequency model, \textit  {BVU}$=$ Bayesian value updating model, \textit  {BASE}$=$ Baseline model.\relax }}{3}{figure.caption.7}\protected@file@percent }
\newlabel{fig:study1_model_weights}{{1}{3}{Evidence for the models for individual participants. \textit {RF}$=$ relative frequency model, \textit {BVU}$=$ Bayesian value updating model, \textit {BASE}$=$ Baseline model.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Parameter Estimates of Winning Models, \textit  {M (SD)}\relax }}{4}{table.caption.9}\protected@file@percent }
\newlabel{tab:study1_parameter}{{2}{4}{Parameter Estimates of Winning Models, \textit {M (SD)}\relax }{table.caption.9}{}}
\@writefile{loc}{\ChangesListline {added}{Added}{The qualitative fit between the models and the data is shown in Figure \ref  {fig:ind_fits1}, which plots best-fitting model predictions against observed evaluations. It shows that the models generally capture the data well (mean $r\textsubscript  {pred,obs} = 0.71$), except in four cases, where even the winning model fails to resemble the data (participants number 1924385, with $r\textsubscript  {pred,obs} < 0.40$). For these cases, for whom the winning model is the Bayesian updating model, the model must be rejected because of qualitative mis-fit.}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \textit  {BVU}$=$ Bayesian value updating model, \textit  {RF}$=$ relative frequency model, \textit  {BASE}$=$baseline model.\relax }}{5}{figure.caption.10}\protected@file@percent }
\newlabel{fig:ind_fits1}{{2}{5}{Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \textit {BVU}$=$ Bayesian value updating model, \textit {RF}$=$ relative frequency model, \textit {BASE}$=$baseline model.\relax }{figure.caption.10}{}}
\@writefile{loc}{\ChangesListline {added}{Added}{The cognitive modeling resuls thus show that most participants used a Bayesian strategy, and some used a relative-frequency strategy. This strategy heterogeneity helps understanding the behavioral null finding---that sample size seemed to have no effect on valuations---that were observed at the aggregate level (Table \ref  {tab:means_study1}). The aggregate analysis fails to take the individual differences in learning strategies into account, while participants are best described by a mixture of strategies. Moreover, the aggregate analysis also fails to account for differences in the prior beliefs about gain probabilities. Depending on the prior belief, the Bayesian value updating (BVU) model predicts either a decrease or an increase in valuations with increasing sample size. The next analysis will focus on these differences.}{5}}
\citation{SingmannForthcoming}
\@writefile{loc}{\ChangesListline {added}{Added\nobreakspace  {}(jj)}{ The Bayesian model predicts that sample size changes the evaluations differently as a function of prior beliefs. Participants with a gain prior---initially believing that gains are more likely than zero-outcomes---should decrease the evaluations of \$-bets as sample sizes increase, because participants overwrite their priors through sampling and learn that gains of \$-bets are less likely than zero-outcomes. By contrast, participants with a zero-outcome prior---initially believing that zero-outcomes are more likely than gains---should increase their evaluations of p-bets as sample size increases, because they learn that gains of p-bets are more likely than zero-outcomes (see the probabilities in Table\nobreakspace  {}\ref  {table:Lotteries}). }{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Mean evaluation (standardized to 0 - 1) by winning model and prior beliefs of the BVU model. \textit  {BVU}$=$Bayesian value updating model, \textit  {RF}$=$ Relative frequency model. Error bars indicate standard errors. \textit  {\$-bet}: low-probability high-outcome gambles, \textit  {p-bet}: high-probability low-outcome gambles. Sample sizes (xs, x, m, l), see Table \ref  {tab:Lotteries}. \textit  {n=16, 13, 6} denotes the number of participants best-described by the respective models.\relax }}{7}{figure.caption.11}\protected@file@percent }
\newlabel{fig:qual1}{{3}{7}{Mean evaluation (standardized to 0 - 1) by winning model and prior beliefs of the BVU model. \textit {BVU}$=$Bayesian value updating model, \textit {RF}$=$ Relative frequency model. Error bars indicate standard errors. \textit {\$-bet}: low-probability high-outcome gambles, \textit {p-bet}: high-probability low-outcome gambles. Sample sizes (xs, x, m, l), see Table \ref {tab:Lotteries}. \textit {n=16, 13, 6} denotes the number of participants best-described by the respective models.\relax }{figure.caption.11}{}}
