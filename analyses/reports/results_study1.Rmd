---
title: ""
author: "Jana B. Jarecki"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
documentclass: "apa6"
classoption:
  a4paper,
  man,
  floatsintext
header-includes:
  \usepackage{natbib}
  \usepackage{threeparttable}
  \usepackage{booktabs}
  \shorttitle{test}
  \usepackage{setspace}
  \AtBeginEnvironment{tabular}{\singlespacing}
  \usepackage{times}
  \usepackage{changes}
  \definechangesauthor[name={JJ}, color=orange]{jj}
  \usepackage{upgreek}
  \AtBeginDocument{\let\maketitle\relax}
---

```{r setupknitr, include=0, cache=0}
  source("setup_knitr.R")
```

```{r setupR, include=0, cache=0}
  source("./code/setup_fig.R")
  library(papaja); library(cogscimodels); library(data.table); library(BayesFactor); library(brms)
  options(papaja.na_string = "--")
  niter <- 1000
  study <- 1
```

```{r, load_data, cache=0, include=0}
  # Read the data
  d <- fread(sub("x", study, "../data/processed/studyx.csv"))
  N <- length(unique(d$id))
  # Read the cognitive model fitting results
  fits <- readRDS(sub("x", study, "./code/studyx_cognitive_models_fit.rds"))  
```

```{r, run_analyses_scripts, cache=0, include=0}
# Table comparing DfE and DfD choices
  source("./code/tab2.R", chdir = TRUE)
  # Run all inferential statistical analyses  
  source("./code/1-statistical-analysis.R", chdir = TRUE)
  # Run the cognitive modeling analysis
  source("./code/3-modeling-analysis.R")
```

\subsubsection{Evaluations of gambles and sample sizes}
Table \ref{tab:means_study1} summarizes the evaluations of the gambles by format (description vs. experience) and sample size (xs, s, m, l). For instance, across sample sizes xs, s, m, and l, the participants evaluated gamble 4 with an average of $`r M[[4]][, M]`$ (respectively). Across gambles, the evaluations were not influenced by sample sizes; a statistical model comparison among analyses of variance (ANOVA) confirmed this, an ANOVA of the valuations with the predictors gamble type and gamble expected value (Model 0) outperformed models with the additional predictors sample size ($BF_{01} `r BF_fe_fess`$) and a sample-size x gamble-type interaction ($BF_{02} `r BF_fe_int`$, all models had by-participant random effects). Although the \$-bets and p-bets had the same expected values, the \$-bet gambles  were evaluated higher ($`r d[gambletype == "$-bet", paste_msd(value, label=TRUE)]`$) than the p-bet gambles ($`r d[gambletype == "p-bet", paste_msd(value, label=TRUE)]`$), $BF `r BF_gambletype`$ in favor of an ANOVA that predicts valuations as a function of gamble type  over an ANOVA without the predictor gamble type (both models include by-participant and by-expected-value random effects).


The valuations in the experience condition were not driven by primacy or recency: neither the outcomes that participants sampled in the first half of the sampling phase nor those sampled during the second half of the sampling phase predict the observed valuations. A linear model of the evaluations with the predictor gamble type ($\mathrm{M}\textsubscript{0}$) outperformed a model with the additional predictor mean of the first half of samples ($BF\textsubscript{01} = 16.6$) and a model with the additional predictor mean of the second half of samples ($BF\textsubscript{02} = 6.7$).


\added[id=jj]{\textit{Description versus experience. }
Participants' valuations from description and experience differed for most of the gambles and sample sizes (Table \ref{tab:means_study1}, rightmost column). The \$-bets were evaluated higher based on experience ($`r dd$V1[1]`$) compared to description ($`r dd$V1[3]`$). By contrast p-bets were evaluated lower on the basis of experience ($`r dd$V1[2]`$) compared to description ($`r dd$V1[4]`$), $BF `r BF_cond_gambletype`$ in favor of an ANOVA including a gamble-type x condition interaction over a model with only the main effects. Thus, we found a D--E gap that differs from the classic D--E gap observed in choice paradigms. In Study 1, participants valued gambles as if they overweighted rare events from description \textit{and} experience. This effect was even stronger when participants made valuations from experience.}


```{r, means_study1}
  # format the Bayes Factor such that BF > 1000 is displayed as > 1000
  M <- lapply(M, function(x) cbind(x[, 1:6], replace(round(x[, 7], 0), round(x[, 7], 0) > 1000, ">1000")))
  apa_table(M
    , caption = "Valuations of Gambles in Study 1"
    , col.names = c("Condition", 'Sample size category', 'Sample size', '\\textit{Med}', '\\textit{M}', 'D--E', 'D--E:$BF\\textsubscript{10}$')
    , align = c('l', rep('c', 4), 'r', 'r')
    , digits = c(0,0,0,2,2,2,0)
    , note = '\\textit{M} = mean, \\textit{Med} = median, D--E = difference between mean description-based valuations and experience-based valuations, $BF\\textsubscript{10}$ = Bayes Factor quantifying the evidence for a linear model $\\mathrm{M}\\textsubscript{1}$ predicting that valuations differ between description and experience over a linear model $\\mathrm{M}\\textsubscript{0}$ predicting no such differences; both models models contain a by-participant random effect. Gambles IDs 1, 2, and 3 are \\$-bets; Gamble IDs 4, 5, and 6 are p-bets.'
    , escape = FALSE
    )
```

At the aggregate level, sample size seems to not influence the evaluations of gambles in our decision from experience paradigm, but the subsequent cognitive modeling analyses will show a more nuanced picture.

\subsubsection{Cognitive modeling}
By modeling participant's valuations with the two models we proposed, it is possible to examine the effect of sample size on value judgments. We tested how good the \added[id=jj]{relative frequency} (RF) model and the
\added[id=jj]{Bayesian value updating} (BVU) model represent participants' value judgments of gambles. To check the psychological plausibility of both models we also checked if both models outperform a baseline model, which predicts a constant evaluation equal to the mean individual evaluation.

\textit{Modeling Procedure.} 
To estimate the models, the observed and predicted evaluations were normalized to a common range between 0 and 1 (by division through the gain of the presented gamble). Maximum likelihood was used to estimate the parameters of the models at the participant level, assuming that observations follow a truncated normal distribution around the model predictions (truncation between 0 and 1) with a constant standard deviation ($\sigma$), that was estimated by participant ($0 < \sigma \leq 1$). \added[id=jj]{Therefore, the relative frequency model had two free parameters, namely $\sigma$ and the power utility exponent $\tau$ ($0 \leq \tau \leq 20$). The Bayesian value updating model had four free parameters, the prior belief parameter for gain outcomes $\alpha_0$ ($0 \leq \alpha_0 \leq 2$), the learning rate $\delta$ ($0 \leq \delta \leq 10$), $\tau$ and $\sigma$, with the prior on zero outcomes constrained to $\beta_0 = 2-\alpha_0$. The baseline model had two free parameters, the mean evaluation $\mu$ and $\sigma$. We estimated the parameters using an augmented Lagrange multiplier method \citep[Rsolnp package, version 1.16]{Ghalanos2015}. Models were compared based on evidence strength and BIC weights from the Bayesian information criterion (BIC) \cite[evidence in favor of a model compared to the individually best-fitting model][]{Kass1995, Lewandowsky2011}. Higher weights indicate stronger evidence for a model.}



```{r, study1_model_fit}
  bic <- setNames(gof[, round(mean(BIC)), by=model]$V1, c("base", "rf", "bvu"))
```

We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.

\textit{Quantitative Model Fit.}
\added[id=jj]{The Bayesian value updating model described the majority of the participants best (`r winners["bvu"]` of `r N`; `r round(winners["bvu"]/N*100)`\%). The relative frequency model described `r winners["rf"]` participants best (`r round(winners["rf"]/N*100)`\%); the baseline model described `r winners["base"]` participants best. Figure \ref{fig:fig2} shows the evidence strength for the models by participant. The models' mean Bayesian information criterion across all participants equaled BIC\textsubscript{BVU}$= `r bic["bvu"]`$, BIC\textsubscript{RF}$= `r bic["rf"]`$, and BIC\textsubscript{BASE}$= `r bic["base"]`$ (lower values indicate better fit).}


```{r fig_modelfit, echo=0, cache=0}
  knitr::read_chunk("./code/fig2.R")
```

```{r fig2, ref.label="fig_modelfit", fig.asp=0.6, cache=0, fig.cap = "Study 1: Evidence for the models for individual participants. \\textit{RF}$=$ relative frequency model, \\textit{BVU}$=$ Bayesian value updating model, \\textit{BASE}$=$ Baseline model."}
```


```{r study1_par}
  M_alpha_bvu <- parameter[winner=="bvu" & par=="rp", mean(val)]
  M_alpha_rf  <- parameter[winner=="rf" & par=="rp", mean(val)]
  M_thetag_bvu <- parameter[winner=="bvu" & par=="count_x", mean(val)]
  M_theta0_bvu <- parameter[winner=="bvu" & par=="count_0", mean(val)]
  M_delta_bvu <- parameter[winner=="bvu" & par=="delta", mean(val)]
  prior_p_gain = round(M_thetag_bvu / (M_theta0_bvu + M_thetag_bvu), 2)
  # T-test of alpha parameter (power-utility exponent) called "rp" here
  bf = ttestBF(parameter[winner=="bvu" & par=="rp", val], parameter[winner=="rf" & par=="rp", val])
```

\added[id=jj]{
The estimated parameters of the best-fitting models are shown in Table \ref{tab:study1_parameter}. The power utility exponent ($\tau$) did not differ substantially for the participants that were best described by the Bayesian value updating (BVU) model ($M_{\tau}= `r M_alpha_bvu`$) and those best described by a relative frequency model ($M_{\tau}=`r M_alpha_rf`$), $\Delta$ `r apa_print(bf)$full_result`. The prior beliefs about the probability of gains was $`r prior_p_gain`$ (resulting from estimated prior parameters $\alpha_0 = `r M_thetag_bvu`$, $\beta_0 = `r M_theta0_bvu`$) for the participants best described by the BVU model. The learning rate of the mean participant that was best described by the BVU model was $M_{\delta}=`r M_delta_bvu`$ (a model with values $>$ 1 revises its priors faster than optimal Bayesian, 1 is optimal Bayesian, $<$ 1 is conservative compared to optimal Bayesian). The liberal learning rate is unusual given that previous work has found conservative learning \citep{Edwards1967,Tauber2017}. This fast learning in our task can be explained by that participants repeatedly sampled from the same set of gambles.
}

```{r, study1_parameter}
  parameter[,
     winner_n := paste0(factor(winner, levels = model_levels, labels = model_labels), " (\\textit{n}$=$", ..winners[winner], ")"),
    by=winner]
  tab <- dcast(parameter, winner_n ~ par, paste_msd, value.var = "val")
  papaja::apa_table(tab[c(2,3,1), c("winner_n", "rp", "delta", "count_x","m", "sigma")]
      , caption = "Study 1: Parameter Estimates of Winning Models, \\textit{M (SD)}"
      , col.names = c("Winning Model","$\\alpha$", "$\\delta$", "$\\theta_G$","$\\mu$","$\\sigma$")
      , align = c("l", rep("c", 5)),
      , note = "\\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model, \\textit{BASE}$=$baseline model. Parameters denote: $\\tau =$ power utility exponent, $\\alpha_0$ gain prior, $\\mu=$ mean evaluation, $\\sigma$ standard deviation."
      , escape = FALSE
      )
```

```{r}
  cutoff <- 0.4
  m_cor <- d[condition == "experience", cor(pred, value), by = id][, mean(V1)]
  ids_cutoff <- d[condition == "experience", cor(pred, value), by = id][V1 < cutoff]$id
```

\textit{Qualitative Model Fit.}
The qualitative fit between the models and the data is shown in Figure \ref{fig:fig3}, which plots the predictions of the best-fitting models against the observed evaluations by participant. The models generally describe the data well \added[id=jj]{($M r\textsubscript{pred,obs} = `r m_cor`$), except in four cases, where even the winning model fails to resemble the data qualitatively (participants `r sort(ids_cutoff)`, with $r\textsubscript{pred,obs} < `r cutoff`$).}\footnote{As robustness check we repeated the model comparison with subjective probability weighting, using Prelec’s \citeyear{Prelec1998} single parameter weighting function. This weighting function incorporates non-linearities in the perception of probabilities. The quantitative results of the probability weighting model and the utility model favored a utility model without probability weighting. Thus, the results suggest that in the paradigm that we studied people's perception of the probabilities of outcomes are relative accurate and best described by a linear probability function.}


```{r, include=0}
knitr::opts_chunk$set(out.width = "\\textwidth", fig.height = 9, fig.width = 7)
```

```{r fig_dots, echo=0, cache=0}
  knitr::read_chunk("./code/fig3.R")
```

```{r fig3, ref.label = "fig_dots", cache=0, fig.cap = "Study 1: Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). Darker color indicates overlapping points. \\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model, \\textit{BASE}$=$baseline model."}
```

\added[id=jj]{
The cognitive modeling results thus show that most participants were best described by a Bayesian strategy, and a minority by a relative-frequency strategy. This strategy heterogeneity helps understanding the behavioral null finding---that sample size seemed to have no effect on valuations---that were observed at the aggregate level (Table \ref{tab:means_study1}). The statistical aggregate analysis fails to take the individual differences in learning strategies into account, while participants are best described by a mixture of strategies. Moreover, the aggregate statistical analysis without the cognitive modeling also fails to account for differences in the prior beliefs about gain probabilities.
}