---
title: ""
author: "Jana B. Jarecki"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
documentclass: "apa6"
classoption:
  a4paper,
  man,
  floatsintext
header-includes:
  \usepackage{natbib}
  \usepackage{threeparttable}
  \usepackage{booktabs}
  \shorttitle{test}
  \usepackage{setspace}
  \AtBeginEnvironment{tabular}{\singlespacing}
  \usepackage{times}
  \usepackage{changes}
  \definechangesauthor[name={JJ}, color=orange]{jj}
  \usepackage{upgreek}
  \AtBeginDocument{\let\maketitle\relax}
---

```{r setupknitr, include=0, cache=0}
  source("setup_knitr.R")
```

```{r setupR, include=0, cache=0}
  source("./code/setup_fig.R")
  library(papaja); library(cogscimodels); library(data.table); library(BayesFactor); library(brms)
  options(papaja.na_string = "--")
  niter <- 1000
  study <- 1
```

```{r, load_data, cache=0, include=0}
  # Read the data
  d <- fread(sub("x", study, "../data/processed/studyx.csv"))
  N <- length(unique(d$id))
  # Read the cognitive model fitting results
  fits <- readRDS(sub("x", study, "./code/studyx_cognitive_models_fit.rds"))  
```

```{r, run_analyses_scripts, cache=0, include=0}
# Table comparing DfE and DfD choices
  source("./code/tab2.R", chdir = TRUE)
  # Run all inferential statistical analyses  
  source("./code/1-statistical-analysis.R", chdir = TRUE)
  # Run the cognitive modeling analysis
  source("./code/3-modeling-analysis.R")
```

\subsubsection{Evaluations of gambles and sample sizes}
Table \ref{tab:means_study1} summarizes the evaluations of the gambles by format (description vs. experience) and sample size (xs, s, m, l). For instance, across sample sizes xs, s, m, and l, our participants evaluated gamble 4 with an average of $`r M[[4]][, M]`$ (respectively). Across gambles, the evaluations were not influenced by sample sizes, which a model comparison among analyses of variance (ANOVA) of the judgments confirmed: an ANOVA with the predictors gamble type and gamble expected value (Model 0) outperformed models with the additional predictors sample size ($BF_{01} `r BF_fe_fess`$) and a sample-size x gamble-type interaction ($BF_{02} `r BF_fe_int`$, all models had by-participant random effects). The \$-bet gambles  were evaluated higher ($`r d[gambletype == "$-bet", paste_msd(value, label=TRUE)]`$) than the p-bet gambles ($`r d[gambletype == "p-bet", paste_msd(value, label=TRUE)]`$), although the \$-bets and p-bets had the same expected values, $BF `r BF_gambletype`$ in favor of an ANOVA that predicts evaluations as a function of gamble type  over an ANOVA without the predictor gamble type (both models include by-participant and by-expected-value random effects).


The evaluations in the experience condition were not driven by primacy or recency: neither the outcomes that participants sampled in the first half of the sampling phase nor those sampled during the second half of said phase predict the observed evaluations. A linear model of the evaluations with the predictor gamble type ($\mathrm{M}\textsubscript{0}$) outperformed a model with the additional predictor mean of the first half of samples ($BF\textsubscript{01} = 16.6$) and a model with the additional predictor mean of the second half of samples ($BF\textsubscript{02} = 6.7$)


\textit{Description versus experience.}
Participants' valuations from description and experience differed for most of the gambles and sample sizes (Table \ref{tab:means_study1}, rightmost column). The \$-bets were evaluated higher based on experience ($`r dd$V1[1]`$) compared to description ($`r dd$V1[3]`$). By contrast p-bets were evaluated lower from experience ($`r dd$V1[2]`$) compared to description ($`r dd$V1[4]`$), $BF `r BF_cond_gambletype`$ in favor of an ANOVA including a gamble-type x condition interaction over a model with only the main effects. Thus, we found a D--E gap that differs from the classic D--E gap observed in choice paradigms. In our study, participants valued gambles as if they overweighted rare events from description \textit{and} experience. This effect was even stronger when people made valuations from experience.


```{r, means_study1}
  # format the Bayes Factor such that BF > 1000 is displayed as > 1000
  M <- lapply(M, function(x) cbind(x[, 1:6], replace(round(x[, 7], 0), round(x[, 7], 0) > 1000, ">1000")))
  apa_table(M
    , caption = "Valuations of Gambles in Study 1"
    , col.names = c("Condition", 'Sample size category', 'Sample size', '\\textit{Med}', '\\textit{M}', 'D--E', 'D--E:$BF\\textsubscript{10}$')
    , align = c('l', rep('c', 4), 'r', 'r')
    , digits = c(0,0,0,2,2,2,0)
    , note = '\\textit{M} = mean, \\textit{Med} = median, D--E = difference between mean description-based valuations and experience-based valuations, $BF\\textsubscript{10}$ = Bayes Factor quantifying the evidence for a linear model $\\mathrm{M}\\textsubscript{1}$ predicting that valuations differ between description and experience over a linear model $\\mathrm{M}\\textsubscript{0}$ predicting no such differences; both models models contain a by-participant random effect. Gambles IDs 1, 2, and 3 are \\$-bets; Gamble IDs 4, 5, and 6 are p-bets.'
    , escape = FALSE
    )
```

At the aggregate level, sample size seems to not influence the evaluations of gambles in our decision from experience paradigm, but the subsequent cognitive modeling analyses will show a more nuanced picture.

\subsubsection{Cognitive modeling}
We used computational modeling to analyze the role of sample size in value judgments more closely, comparing the performance of the \added[id=jj]{relative frequency} (RF) model and the \added[id=jj]{Bayesian value updating} (BVU) model. \added[id=jj]{The models were compared to a baseline model, which predicts a constant evaluation equal to the mean individual evaluation (sensible models are expected to outperform this baseline model).}

\textit{Modeling Procedure.} 
To fit the model parameters, the observed and predicted evaluations were normalized to a common range between 0 and 1 (by division through the gain of the presented gamble). Maximum likelihood was used to estimate the free model parameters at the participant level, assuming that observations follow a truncated normal distribution around the model predictions (truncation between 0 and 1) with a constant standard deviation ($\sigma$), that was estimated by participant ($0 < \sigma \leq 1$). \added[id=jj]{Therefore, the relative frequency model had two free parameters, namely $\sigma$ and the power utility exponent $\alpha$ ($0 \leq \alpha \leq 20$). The Bayesian value updating model had four free parameters, the gain prior $\theta_G$ ($0 \leq \theta_G \leq 2$), the learning rate $\delta$ ($0 \leq \delta \leq 10$), $\alpha$ and $\sigma$, with the prior on zero outcomes constrained to $\theta_0=2-\theta_G$. The baseline model had two free parameters, the mean evaluation $\mu$ and $\sigma$. We estimated the parameters using an augmented Lagrange multiplier method \citep[Rsolnp package, version 1.16]{Ghalanos2015}. Models were compared based on evidence strength and BIC weights from the Bayesian information criterion (BIC) \cite[evidence in favor of a model compared to the individually best-fitting model][]{Kass1995, Lewandowsky2011}. Higher weights indicate stronger evidence for a model.}



```{r, study1_model_fit}
  bic <- setNames(gof[, round(mean(BIC)), by=model]$V1, c("base", "rf", "bvu"))
```

We will first outline the quantitative model fit, followed by the qualitative model fit, and lastly analyze the effects of sample size given the cognitive strategies.

\textit{Quantitative Model Fit.}
\added[id=jj]{The Bayesian value updating model described the majority of the participants best (`r winners["bvu"]` of `r N`; `r round(winners["bvu"]/N*100)`\%). The relative frequency model described `r winners["rf"]` participants best (`r round(winners["rf"]/N*100)`\%); the baseline model described `r winners["base"]` participants best. Figure \ref{fig:fig2} shows the evidence strength for the models by participant. The models' mean Bayesian information criterion across all participants equaled BIC\textsubscript{BVU}$= `r bic["bvu"]`$, BIC\textsubscript{RF}$= `r bic["rf"]`$, and BIC\textsubscript{BASE}$= `r bic["base"]`$ (lower values indicate better fit).}


```{r, echo=FALSE, cache=FALSE}
  knitr::read_chunk("./code/fig2.R")
```

```{r fig2, fig.asp=0.6, cache=FALSE, fig.cap = "Evidence for the models for individual participants. \\textit{RF}$=$ relative frequency model, \\textit{BVU}$=$ Bayesian value updating model, \\textit{BASE}$=$ Baseline model."}
```


```{r study1_par}
  M_alpha_bvu <- parameter[winner=="bvu" & par=="rp", mean(val)]
  M_alpha_rf  <- parameter[winner=="rf" & par=="rp", mean(val)]
  M_thetag_bvu <- parameter[winner=="bvu" & par=="count_x", mean(val)]
  M_theta0_bvu <- parameter[winner=="bvu" & par=="count_0", mean(val)]
  M_delta_bvu <- parameter[winner=="bvu" & par=="delta", mean(val)]
  prior_p_gain = round(M_thetag_bvu / (M_theta0_bvu + M_thetag_bvu) * 100)
  # T-test of alpha parameter (power-utility exponent) called "rp" here
  bf = ttestBF(parameter[winner=="bvu" & par=="rp", val], parameter[winner=="rf" & par=="rp", val])
```

\added[id=jj]{
The estimated parameter of the best-fitting models, which Table \ref{tab:study1_parameter} summarizes, reveal that the power utility exponent ($\alpha$) did not differ for the participants that were described by a Bayesian value updating strategy ($M_{\alpha}= `r M_alpha_bvu`$) and those using a relative frequency strategy ($M_{\alpha}=`r M_alpha_rf`$), $\Delta$ `r apa_print(bf)$full_result`. Participants using the Bayesian strategy had, on average, a prior belief that gains occur with `r prior_p_gain`\% (gain prior $\theta_G = `r M_thetag_bvu`$; zero-outcome prior $\theta_0 = `r M_theta0_bvu`$). Also, their estimated learning rate $\delta$ was anti-conservative ($M_{\delta}=`r M_delta_bvu`$; values $>$ 1 are liberal, 1 is optimal Bayesian, $<$ 1 is conservative learning). The liberal learning rate is unusual given that previous work has found conservative learning \citep{Edwards1967,Tauber2017}. The liberal learning in our data can be explained by that participants repeatedly sampled from the same set of gambles.
}

```{r, study1_parameter}
  parameter[,
     winner_n := paste0(factor(winner, levels = model_levels, labels = model_labels), " (\\textit{n}$=$", ..winners[winner], ")"),
    by=winner]
  tab <- dcast(parameter, winner_n ~ par, paste_msd, value.var = "val")
  papaja::apa_table(tab[c(2,3,1), c("winner_n", "rp", "delta", "count_x","m", "sigma")]
      , caption = "Parameter Estimates of Winning Models, \\textit{M (SD)}"
      , col.names = c("Winning Model","$\\alpha$", "$\\delta$", "$\\theta_G$","$\\mu$","$\\sigma$")
      , align = c("l", rep("c", 5)),
      , note = "\\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model, \\textit{BASE}$=$baseline model. Parameters denote: $\\alpha=$ power utility exponent, $\\theta_G$ gain prior, $\\mu=$ mean evaluation, $\\sigma$ standard deviation."
      , escape = FALSE
      )
```

```{r}
  cutoff <- 0.4
  m_cor <- d[condition == "experience", cor(pred, value), by = id][, mean(V1)]
  ids_cutoff <- d[condition == "experience", cor(pred, value), by = id][V1 < cutoff]$id
```

\textit{Qualitative Model Fit.}
The qualitative fit between the models and the data is shown in Figure \ref{fig:fig5}, which plots the predictions of the best-fitting models against the observed evaluations by participant. The models generally describe the data well \added[id=jj]{($M r\textsubscript{pred,obs} = `r m_cor`$), except in four cases, where even the winning model fails to resemble the data qualitatively (participants `r sort(ids_cutoff)`, with $r\textsubscript{pred,obs} < `r cutoff`$).} For these cases, for whom the winning model is the Bayesian updating model, the model must be rejected because of qualitative mis-fit.\footnote{As robustness check we repeated the model comparison with subjective probability weighting, using Prelecâ€™s single parameter weighting function. This weighting function incorporates non-linearities in the perception of probabilities. However, the quantitative results of the probability weighting model and the utility model, we favored a utility model without probability weighting.}


```{r include = 0}
knitr::opts_chunk$set(out.width = "\\textwidth", fig.height = 9, fig.width = 7)
```

```{r, echo=0, cache=0}
  knitr::read_chunk("./code/fig5.R")
```
```{r fig5, cache=0, fig.cap = "Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model, \\textit{BASE}$=$baseline model."}
```

\added[id=jj]{
The cognitive modeling results thus show that most participants were described by a Bayesian strategy, and a minority by a relative-frequency strategy. This strategy heterogeneity helps understanding the behavioral null finding---that sample size seemed to have no effect on valuations---that were observed at the aggregate level (Table \ref{tab:means_study1}). The aggregate analysis fails to take the individual differences in learning strategies into account, while participants are best described by a mixture of strategies. Moreover, the aggregate analysis also fails to account for differences in the prior beliefs about gain probabilities.
}