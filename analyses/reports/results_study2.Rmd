---
title: ""
author: "Jana B. Jarecki"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
documentclass: "apa6"
classoption:
  a4paper,
  man,
  floatsintext
header-includes:
  \usepackage{natbib}
  \usepackage{threeparttable}
  \usepackage{booktabs}
  \shorttitle{test}
  \usepackage{setspace}
  \AtBeginEnvironment{tabular}{\singlespacing}
  \usepackage{times}
  \usepackage{changes}
  \definechangesauthor[name={JJ}, color=orange]{jj}
  \usepackage{upgreek}
  \AtBeginDocument{\let\maketitle\relax}
---

```{r setupknitr, include=0, cache=0}
# Run the setup in a separate chunk at the very beginning
 # Do not cache this chunk where you load libraries and set options
 # because the code has side effects
  source("setup_knitr.R")
```

```{r setupR, include=0, cache=0}
  source("./code/setup_fig.R")
  library(papaja); library(cogscimodels); library(data.table); library(BayesFactor); library(brms)
  options(papaja.na_string = "--")
  niter <- 1000
  study <- 2
```

```{r, load_data, cache=1, include=0}
  # Read the data
  d <- fread(sub("x", study, "../data/processed/studyx.csv"))
  N <- length(unique(d$id))
  # Read the cognitive model fitting results
  fits <- readRDS(sub("x", study, "./code/studyx_cognitive_models_fit.rds")) 
```

```{r, run_analyses_scripts, cache=0, include=0}
  # Table comparing DfE and DfD choices
  source("./code/tab2.R", chdir = TRUE)
  # Run all inferential statistical analyses  
  source("./code/1-statistical-analysis.R", chdir = TRUE)
  # Run the cognitive modeling analysis
  source("./code/3-modeling-analysis.R")
```

\subsubsection{Valuation of gambles across sample sizes}
The average valuations of the gambles given the sample sizes in Study 2 resemble those in Study 1 (Table \ref{tab:means_study2}). Larger sample sizes did not lead to systematic changes in evaluations of gambles; an ANOVA with the predictors gamble type and gamble expected value ($M_0$) outperformed one with the added predictor sample size ($BF_{01} `r BF_fe_fess`$) and a sample size x gamble type interaction ($BF_{02} `r BF_fe_int`$; models with by-participant random effects). \added[id=jj]{Unlike in Study 1 there was no substantial difference between the evaluation of \$-bet gambles  ($`r d[gambletype == "$-bet", paste_msd(value, label=TRUE)]`$) and p-bet gambles ($`r d[gambletype == "p-bet", paste_msd(value, label=TRUE)]`$); the data equally supported a model including gamble type as predictor ($M_0$) and one without gamble type as predictor, $BF_{01} `r 1/BF_gambletype`$ (both models include by-participant and by-expected-value random effects and use the valuations as the dependent variable).}

\textit{Description versus experience.}
The valuations from description differed from the valuations from experience for most of the gambles and sample sizes (Table \ref{tab:means_study2}, rightmost column). However, the finding is, especially for
p-bets, less pronounced than in Study 1. Unlike in Study 1, the \$-bets were evaluated similar from experience ($`r dd$V1[1]`$) and description ($`r dd$V1[3]`$), and the p-bets were evaluated similarly from experience ($`r dd$V1[2]`$) and description ($`r dd$V1[4]`$), the data did neither support an ANOVA with gamble-type x condition interaction ($M_0$) nor a main-effects model, $BF_{01} `r BF_cond_gambletype`$.

```{r, means_study2}
  # format the Bayes Factor such that BF > 1000 is displayed as > 1000
  M <- lapply(M, function(x) cbind(x[, 1:6], replace(round(x[, 7], 0), round(x[, 7], 0) > 1000, ">1000")))
  apa_table(M
    , caption = "Valuations of Gambles in Study 1"
    , col.names = c("Condition", 'Sample size category', 'Sample size', '\\textit{Med}', '\\textit{M}', 'D--E', 'D--E:$BF\\textsubscript{10}$')
    , align = c('l', rep('c', 4), 'r', 'r')
    , digits = c(0,0,0,2,2,2,0)
    , note = '\\textit{M} = mean, \\textit{Med} = median, D--E = difference between mean description-based valuations and experience-based valuations, $BF\\textsubscript{10}$ = Bayes Factor quantifying the evidence for a linear model $\\mathrm{M}\\textsubscript{1}$ predicting that valuations differ between description and experience over a linear model $\\mathrm{M}\\textsubscript{0}$ predicting no such differences; both models models contain a by-participant random effect. Gambles IDs 1, 2, and 3 are \\$-bets; Gamble IDs 4, 5, and 6 are p-bets.'
    , escape = FALSE
    )
```


\subsubsection{Cognitive modeling}
To gain a better insight into how sample size affects the evaluations of the gambles we estimated the cognitive models following the same procedure as in Study 1.

```{r, study2_model_fit}
  bic <- setNames(gof[, round(mean(BIC)), by=model]$V1, c("base", "rf", "bvu"))
```

\textit{Quantitative Model Fit.}
The Bayesian value updating model described slightly more than half of the participants best (`r winners["bvu"]` of `r N`; `r round(winners["bvu"]/N*100)`\%). The relative frequency model described `r winners["rf"]` participants best (`r round(winners["rf"]/N*100)`\%). The evidence strength of the models is shown in Figure \ref{fig:fig2_2}. The models' mean Bayesian information criterion across all participants equaled BIC\textsubscript{BVU}$= `r bic["bvu"]`$, BIC\textsubscript{RF}$= `r bic["rf"]`$, and BIC\textsubscript{BASE}$= `r bic["base"]`$ (lower values indicate better fit).




```{r, fig2, echo=0, cache=0}
  knitr::read_chunk("./code/fig2.R")
```

```{r fig2_2, ref.label="fig2", fig.asp=0.6, cache=0, fig.cap = "Study 2: Evidence for the models for individual participants. \\textit{RF}$=$ relative frequency model, \\textit{BVU}$=$ Bayesian value updating model, \\textit{BASE}$=$ Baseline model."}
```

```{r par_study2}
  M_alpha_bvu <- parameter[winner=="bvu" & par=="rp", mean(val)]
  M_alpha_rf  <- parameter[winner=="rf" & par=="rp", mean(val)]
  M_thetag_bvu <- parameter[winner=="bvu" & par=="count_x", mean(val)]
  M_theta0_bvu <- parameter[winner=="bvu" & par=="count_0", mean(val)]
  M_delta_bvu <- parameter[winner=="bvu" & par=="delta", mean(val)]
  prior_p_gain = round(M_thetag_bvu / (M_theta0_bvu + M_thetag_bvu), 2)
  # T-test of alpha parameter (power-utility exponent) called "rp" here
  bf = ttestBF(parameter[winner=="bvu" & par=="rp", val], parameter[winner=="rf" & par=="rp", val])
```

```{r, parameter_study2}
  parameter[,
     winner_n := paste0(factor(winner, levels = model_levels, labels = model_labels), " (\\textit{n}$=$", ..winners[winner], ")"),
    by=winner]
  tab <- dcast(parameter, winner_n ~ par, paste_msd, value.var = "val")
  papaja::apa_table(tab[c(1,2), c("winner_n", "rp", "delta", "count_x", "sigma")]
      , caption = "Study 2: Parameter Estimates of Winning Models, \\textit{M (SD)}"
      , col.names = c("Winning Model","$\\tau$", "$\\delta$", "$\\alpha_0$","$\\sigma$")
      , align = c("l", rep("c", 5)),
      , note = "\\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model. Parameters denote: $\\tau=$ power utility exponent, $\\alpha_0$ gain prior, $\\sigma$ standard deviation."
      , escape = FALSE
      )
```

\added[id=jj]{
The estimated parameters of the models (Table \ref{tab:parameter_study2}) reveal that, similar to Study 1, the power utility exponent did not differ greatly between the participants that were best described by the Bayesian value updating (BVU) model ($M_{\tau}= `r M_alpha_bvu`$) and those best described by the relative frequency model ($M_{\alpha}=`r M_alpha_rf`$), $\Delta$ `r apa_print(bf)$full_result`. The prior belief about the probability of a positive outcome, for participants that were best described the BVU model, was $B = `r prior_p_gain`$ (resulting from $\alpha = `r M_thetag_bvu`$ and $\beta_0 = `r M_theta0_bvu`$) and the associated learning rate $\delta$ was $M_{\delta}=`r M_delta_bvu`$, indicating faster belief updating compared to an optimal Bayesian model.
}



```{r}
  cutoff <- 0.4
  m_cor <- d[condition == "experience", cor(pred, value), by = id][, mean(V1)]
  ids_cutoff <- d[condition == "experience", cor(pred, value), by = id][V1 < cutoff]$id
```

\textit{Qualitative Model Fit.}
Figure \ref{fig:fig5_2} plots the qualitative model fit (best-fitting model predictions against observed evaluations by participant). As in Study 1, the data are generally well-described by the models \added[id=jj]{($M r\textsubscript{pred,obs} = `r m_cor`$), except in four cases (participants `r sort(ids_cutoff)`, with $r\textsubscript{pred,obs} < `r cutoff`$)} where the model must be rejected because of qualitative mis-fit.

```{r include = 0}
knitr::opts_chunk$set(out.width = "\\textwidth", fig.height = 9, fig.width = 7)
```

```{r, echo=0, cache=0}
  knitr::read_chunk("./code/fig5.R")
```
```{r fig5_2, ref.label="fig5", cache=0, fig.cap = "Study 2: Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \\textit{BVU}$=$ Bayesian value updating model, \\textit{RF}$=$ relative frequency model."}
```