---
title: ""
author: "Jana B. Jarecki"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    keep_tex: yes
documentclass: "apa6"
classoption:
  a4paper,
  man,
  floatsintext
header-includes:
  \usepackage{natbib}
  \usepackage{threeparttable}
  \usepackage{booktabs}
  \shorttitle{test}
  \usepackage{setspace}
  \AtBeginEnvironment{tabular}{\singlespacing}
  \usepackage{times}
  \usepackage{changes}
  \definechangesauthor[name={JJ}, color=orange]{jj}
  \usepackage{upgreek}
  \AtBeginDocument{\let\maketitle\relax}
---

```{r setupknitr, include=0, cache=0}
  source("../../manuscript/setup_knitr.R")
```

```{r setupR, include=0, cache=0}
  source("code/setup_fig.R")
  source("code/utilities.R")
  options(papaja.na_string = "--")
  require(data.table)
  require(BayesFactor)
```

```{r loadData, include=0, cache=1}
source("code/4-evaluations-by-prior.R", chdir = TRUE)
```


\added[id=jj]{To test these hypotheses, participants were classified as relative-frequency learners (RF-type; $n=`r d[prior=="no", length(unique(id))]`$), Bayesian learners with prior beliefs that gains are more likely than zero outcomes, $\alpha_0 > \beta_0$ ($n=`r d[prior=="gain", length(unique(id))]`$), and Bayesian learners with prior beliefs that zero outcomes are more likely than gains, $\alpha_0 < \beta_0$ ($n=`r d[prior=="zero", length(unique(id))]`$), based on the individually best-fitting cognitive model. The participants that were best described by the baseline model and that showed qualitative model mis-fit (Figures \ref{fig:fig3} and \ref{fig:fig5}) were excluded ($n=`r 80-N`$).}

\added[id=jj]{Figure \ref{fig:fig6}a shows that the RF-type participants kept their valuations relatively stable independent of sample size whereas the Bayesian participants changed their evaluations with sample size. Those Bayesian participants who were described by a prior belief that zero outcomes were very likely (zero-outcome prior) increased their evaluations of p-bets. Those Bayesian participants who were described by prior belief that positive outcomes were very likely (gain prior) slightly decreased their evaluations of \$-bets, as predicted. Statistical analyses by means of a Bayesian mixed-effects model}\footnote{Regressing the (normalized, within-person z-standardized) evaluations on the predictors sample size category (coded as 1,2,3,4), gamble type, and learner class (BVU-gain-prior, BVU-loss-prior, RF) and interactions of the predictors, with a by-participant random intercept; categorical predictors were effects-coded to facilitate interpretation of interactions. The statistical model used weakly informative priors.} \added[id=jj]{supported a model that includes a sample size x learner type interaction and a gamble type x learner type interaction ($M_1$) over a model without the first interaction term ($BF\textsubscript{10} `r BF_value1`$) and over a model without the second interaction term ($BF\textsubscript{20} `r BF_value2`$). The modal posterior regression coefficient estimates for the effect of sample size on valuation in Bayesian learners with zero-outcome priors who sampled p-bets was positive, $b_\textsubscript{BVU,zero,p-bet}$ `r b_pz`, and for Bayesians with gain priors who sampled \$-bets the estimate was negative, $b_\textsubscript{BVU,gain,\$-bet}$ `r b_dg`, as hypothesized. For RF-type participants and contrary to the RF model predictions, the estimated regression coefficients for the effect of sample size on the valuations showed trends: the modal coefficient for p-bets was $b_\textsubscript{RF,p-bet}$ `r b_pn`, and for \$-bets it was $b_\textsubscript{RF,\$-bet}$ `r b_dn`. This indicates that even those participants best-described by the relative frequency model were influenced by sample size to a small degree.}

```{r, echo=0, cache=0, fig.height = 7, fig.width = 3.5, fig.pos = "H"}
  knitr::read_chunk("./code/fig6.R")
```

```{r fig6, cache=0, fig.height = 6, fig.cap = "Mean evaluations by gamble type and best-fitting cognitive model and prior beliefs of the BVU model. \\textit{BVU}$=$Bayesian value updating model, \\textit{RF}$=$ Relative frequency model. Error bars indicate standard errors. Sample size categories see Table \\ref{table:Lotteries}. Evaluations are scaled to 0-1 and z-standardized at the individual level."}
```

\textit{Predictions about confidence.}
\added[id=jj]{The Bayesian model also predicts that people's confidence in their beliefs should increase with more evidence they gather. Thus Bayesian learners' confidence is expected to increase with sample size. The relative frequency model makes no predictions about confidence. To test this, we classified participants into Bayesian and relative-frequency learners based on the best-fitting model. Mean confidence ratings of Bayesian-type learners, z-standardized within participants, increased only slightly from the extra small sample size ($`r d[samplesizecat == "xs" & model == "bvu", paste_msd(conf_scaled, label = T)]`$), to the small sample size ($`r d[samplesizecat == "s" & model == "bvu", paste_msd(conf_scaled, label = T)]`$), medium sample size ($`r d[samplesizecat == "m" & model == "bvu", paste_msd(conf_scaled, label = T)]`$), and large sample size ($`r d[samplesizecat == "l" & model == "bvu", paste_msd(conf_scaled, label = T)]`$, Figure \ref{fig:fig6}b), but statistical analyses showed that a linear regression model}\footnote{Regressing the (within-person z-standardized) confidence ratings on the predictors sample size category (coded as 1,2,3,4), gamble type, and winning model (BVU, RF), and interactions of the predictors, with a by-participant random intercept; categorical predictors were effects-coded to facilitate interpretation of interactions.} \added[id=jj]{($M_1$) that includes the sample size as predictor performed worse than an intercept-only model ($BF_{01}`r 1/BF_conf_prior`$).}
