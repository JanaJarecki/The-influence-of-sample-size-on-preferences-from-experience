\documentclass[a4paper, man, natbib, floatsintext]{apa6}
\usepackage[maxfloats=25]{morefloats}
\usepackage{standalone}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\graphicspath{{analyses/code/}{analyses/figures/}}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{url}
\usepackage{rotating}
\usepackage[toc,page]{appendix}
\usepackage{color}
\usepackage{subscript}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage{longtable}
\usepackage{times}
\usepackage{upgreek}
\usepackage{setspace}
%\AtBeginEnvironment{tabular}{\onehalfspacing}
%% Revision:
%% Use "final" option to remove all tracking markups
\usepackage[dvipsnames]{xcolor}
\usepackage[authormarkup=none]{changes}
\definechangesauthor[name={JJ}, color=black]{jj}
\definechangesauthor[name={JJ}, color=blue]{jj2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{The influence of sample size on preferences from experience}
\shorttitle{sample size and preference}
\author{Janine C. Hoffart\\Jana B. Jarecki\\Gilles Dutilh\\J\"org Rieskamp}
\affiliation{University of Basel, Department of Psychology, Center for Economic Psychology}
\leftheader{Hoffart, Jarecki, Rieskamp, Dutilh}

\abstract{People often learn from experience about the distribution of outcomes of risky options. Typically, people draw small samples, when they can actively sample information from risky gambles to make decisions. \added[id=jj]{We examine how the size of the sample that people experience in decision from experience affects their preferences between risky options. In two studies (N=40 each) we manipulated the size of samples that people could experience from risky gambles and measured subjective selling prices and the confidence in selling price judgments after sampling.} The results show that, on average, sample size influenced neither the selling prices nor confidence. However, \added[id=jj]{cognitive modeling of individual-level learning showed that most} participants could be classified as Bayesian learners, whereas the minority adhered to a frequentist learning strategy. The observed selling prices of Bayesian learners changed with sample size as predicted by Bayesian principles, whereas sample size \added[id=jj]{affected} the judgments of frequentist learners \added[id=jj]{much less}. These results illustrate the variability in how people learn from sampled information and provide an explanation for why sample size often does not affect judgments.
}

\keywords{D--E gap, valuations from experience, \added[id=jj]{risky gambles, cognitive modeling}}
\authornote{This research was supported by a grant (SNSF \# 143854) from the Swiss National Science Foundation to the third and fourth author. We thank Anita Todd for editing the manuscript. 

Correspondence concerning this article should be addressed to Janine Christin Hoffart, University of Basel, Department of Psychology, Missionsstrasse 62a, 4055 Basel, Switzerland.  E-mail: janine.hoffart@gmail.com}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%% results rtex setup -------------------------------------%
<<setup, echo=FALSE, results = "hide">>=
knit_hooks$set(document = function(x) {
  sub('\\usepackage[]{color}',
'\\usepackage[usenames,dvipsnames]{color}', x, fixed = TRUE)
})
options(digits = 3)
opts_chunk$set(fig.path="../analyses/figures/")
opts_chunk$set(fig.pos = 'H', echo=FALSE)
source("setup_knitr.R")
R1 <- readRDS("results1.rds")
R2 <- readRDS("results2.rds")
@

\maketitle





In everyday life, people often form preferences on the basis of past experiences. The strength of these preferences arguably depends on the amount of previous experience. Someone who has visited a restaurant for many years and has always had positive experiences likely has a stronger positive preference for this restaurant than someone who visited the restaurant only once and had a positive experience. Thus, even if the relative frequency of positive experiences is similar, the strength of preferences can differ depending on the amount of experience. We examined how people's preferences in an experience-based judgment situation under risk evolve as a function of growing experience. 

%In everyday life people often base their preferences on past experiences. How much experience people have made with an option arguably influences how strong people's opinion about this option is. Someone who had made 50 out of 50 good experiences with a restaurant probably has a stronger preference for this restaurant than someone who had made 1 out of 1 good experience with this restaurant although the relative frequency of good experiences is the same in both situations. Here, we study, how people's preferences (i.e. valuations) in an experience-based task evolve as a function of growing experience. 

\section{Search Effort in Experience-Based Tasks and the Description-Experience Gap}
\added[id=jj]{Search effort refers to the sample size that people draw in experience-based risky choice tasks. In these tasks, people learn about the distribution of outcomes of gambles by repeatedly drawing outcomes from a gamble until they feel confident to make a judgment or decision \citep[e.g.,][]{Hertwig2004, Hau2008}.} By contrast, in description-based tasks, people judge risky gambles from numerical, graphical, or textual summaries of the gambles' outcome distributions. Past research found that risk preferences differ between experience- and description-based tasks (the decision-experience gap or D--E gap). In decisions from description, people choose as if they overweight rare events \citep{Kahneman1979}, in decisions from experience, they choose as if they underweight rare events \citep{Hertwig2004}.

People's search effort (size of the sample they draw) in experience-based tasks plays a key role in explaining the D--E gap, because smaller samples cause higher sampling error. Sampling error means that the observed relative frequency in the outcome sequence deviates from the objective outcome probability \citep{Hadar2009}. Drawing more samples diminishes the sampling error, but usually the number of samples drawn by people are not sufficient to allow inferences about the true distribution of a gamble with a skewed outcome distribution. Participants in \cite{Hertwig2004}, for instance, drew a median of 15 outcomes per decision, and with this number of outcomes, the skew of a two-outcome gamble's binomial sampling distribution implies that more people undersample than oversample low-probability outcomes (rare events). Indeed, averaged across trials, in Hertwig et al.'s study, the rare event was undersampled by $78\%$ of the participants. Other studies have reported similar sampling errors \citep[e.g.,][]{Hau2008,Rakow2008}. Explanations for why people often search insufficiently include limits of memory, opportunity costs \citep{Hau2008}, or adaptive choice simplification \citep{Hertwig2010}: Simulations have shown that small samples amplify the differences between the observed means of the outcome distributions of risky options.

\added[id=jj]{To understand the D--E gap, previous work tried to eliminate sampling error in experienced-based risky decision making.} Attempts have involved encouraging participants to draw larger samples \citep{Hau2008}, showing samples that represent the gambles' true probability distributions \citep{Ungemach2009}, showing large numbers of random samples \citep{Hau2008,Hau2010}, describing gambles to match the outcome distribution sampled by other participants \citep{Rakow2008}, and only analyzing trials in which the drawn sample approximates the gambles' true outcome distribution \citep[e.g.,][]{Camilleri2009, Camilleri2011a}. Sometimes the D--E gap diminished \citep{Hau2008,Hau2010, Ungemach2009} or vanished \citep{Gloeckner2012, Camilleri2009,Camilleri2011a,Rakow2008}, but despite these mixed findings, the D--E gap is a robust phenomenon \citep[for a recent meta-analysis, see][]{Wulff2017}. \added[id=jj]{Most of this research focused on sampling error in combination with sample size, because sampling error is more likely in small samples.}

\added[id=jj]{In the present work we examine the role of sample size in experience-based evaluations independently of sampling error. To do so, we manipulated the number of outcomes drawn from a risky options' outcome distribution (sample size) and elicited people's preferences (i.e., selling prices). Our experimental design eliminates the sampling error, by ensuring that the realized outcomes represent the expected relative outcome frequency, so that we can examine the pure effect of sample size on people's preferences for risky options in an experienced-based task.}

\section{How Does Sample Size Influence Preferences?}
The question of how sample size influences judgments has interested psychologists for decades \citep[e.g.,][]{Tversky1971, Griffin1992}. We investigate how sample size influences preferences from experience by contrasting two theories: \textit{the belief in the law of small numbers} \citep{Tversky1971} and \textit{Bayesian updating}. 

The belief in the law of small numbers, which was formulated by \cite{Tversky1971}, is people's belief that it is highly probable that a small sequences of random draws represents the true outcome distribution. When asked to mentally produce a short sequence of random coin tosses, people produced sequences with a relative frequency of heads and tails of $.50$ more often than statistically expected \citep{Tversky1971}. In \cite{Griffin1992}, participants ignored sample size in their judgments of coin biases after seeing sequences of coin tosses of different length, some of which were biased. Participants mainly based their estimates of the probabilities associated with the coins on the observed relative frequency of heads and tails. Surprisingly, participants expressed higher confidence in their judgments when the number of coin tosses was small, \added[id=jj]{whereas from a statistical perspective, confidence should be lower for smaller samples.} Several studies have found that people's inferential \citep[e.g.,][]{Kutzner2016}, perceptual \citep[e.g.,][]{Kvam2016}, and preferential judgments \citep[e.g.,][]{Powell2017} are less sensitive to sample sizes than expected by normative models.

In contrast to the belief in the law of small numbers, Bayesian psychological principles suggest that not only the relative frequency of outcomes but also sample size influences judgments. The Bayesian view offers a normative solution to how people deal with sample sizes. People can treat sample size as a measure of uncertainty, knowing that small samples can lead to a biased representation of the true outcome distribution and larger samples to a more veridical description of the true outcome distribution. Consistently, there is some evidence that people treat sample size as a measure of uncertainty, for instance, when they report preferences for consumer goods \citep{DeMartino2017}, make judgments about group differences \citep{Obrecht2010}, or student performance \citep{Fiedler2002}.

\subsection{Cognitive Models and Hypothesis}
To contrast the Bayesian principles with the belief in the law of small numbers in experience-based probability learning we specified two computational learning models. Before introducing the models, it is crucial to recognize that our experimental methods employ predefined sample sizes and representative sequences. In our experiments, participants sampled a \textit{predefined} number of times from a gamble's outcome distribution and evaluated it by indication of the gamble's selling price. Gambles were two-outcome gamble characterized by a gain and a zero-outcome. Individual gambles were presented with different sample sizes and these sample sequences were \textit{representative} of the gamble's true outcome distribution, which allowed us to isolate the effect of sample size without sampling error. For our task the models were formalized as models of learning the probability with which the nonzero gain outcome occurred for the gambles. (Despite this simplification the models can be extended for cases with different types of outcomes).

We follow an expected utility approach. \added[id=jj]{The subjective expected utility of a risky option is given by the sum of the subjective utilities of the outcomes, weighted by the respective probabilities. We represent the subjective utilities of outcomes by a power utility function, defined as $u(g) = g^{\mathrm{\tau}}$, where $g$ is a gain outcome and $\tau$ is a free risk preference parameter. When the outcome probabilities must be learned, as in decisions from experience, the true probabilities are replaced by the decision makers' beliefs about outcome probabilities. We consider two models of learning the outcome probabilities: A relative frequency (RF) model and a Bayesian value updating (BVU) model, that both formalize people's beliefs about the outcome probabilities after observing outcomes, but follow different psychological mechanism to do so.}

\subsubsection{Relative frequency model}

\added[id=jj]{The \textit{relative frequency (RF) model} forms beliefs about probabilities from relative frequencies. It formalizes how people, who trust in the law of small numbers, estimate probabilities.} The model predicts that after $t$ observations, the belief about the probability of a gain outcome corresponds to the observed relative frequencies of gains
$${B}_{\mathrm{RF}, t}  =  \frac{n_t}{t} ,$$
\added[id=jj]{where $n_t$ represents the number of gains observed until sample number $t$ (including $t$).}

\added[id=jj]{The relative frequency model predicts that changes in sample size do not change the resulting valuations of risky options unless the sample size changes the observed relative frequencies. The relative frequency model does not incorporate prior beliefs about the probabilities of outcomes.}

\subsubsection{Bayesian value updating model}
The \emph{Bayesian value updating (BVU)} model implements a Bayesian perspective on probability learning and the resulting uncertainty from sampling. Following Bayesian principles, beliefs about probabilities are based on the observed frequencies and based on prior beliefs about the probabilities of outcomes. The Bayesian model formalizes the distribution of the beliefs about gain probabilities by a beta distribution ${B}_{\mathrm{BVU}} \sim Beta(\alpha,\beta)$ \cite[for an introduction to Bayesian cognitive models, see][]{Griffiths2008a}. The initial beliefs at $t = 0$ are given by a prior belief distribution $B_{\mathrm{BVU},0} \sim Beta(\alpha_0$, $\beta_0)$, with parameters that can be seen as prior pseudo-observations of gains ($\alpha_0$) and zero-outcomes ($\beta_0$), and $\alpha_0 + \beta_0 = 2$. \added[id=jj]{Upon experiencing a sample with size $t$, the learners update their prior belief to a posterior belief $B_t$ represented as}

$$B_{\mathrm{BVU},t} \sim Beta(\alpha_t, \beta_t),$$
$$\text{with } \alpha_t = \alpha_0 + \delta n_t, \text{ and } \beta_t = \beta_0 + \delta n_0,$$

\noindent\added[id=jj]{where $n_t$ represents the number of gain outcomes and $n_0$ represents the number of zero outcomes observed up to outcome number $t$ (including $t$). The parameter $\delta$ is a learning rate that scales the number of observations and can account for individual differences in the revision of prior beliefs \citep{Edwards1967,Tauber2017,Jarecki2017}.  As point estimate of people's (posterior) beliefs about the probability of the gain outcome, we use the expected value of the posterior distribution, which is given by $\frac{\alpha_t}{\alpha_t + \beta_t}$.}

\added[id=jj]{The learning rate adjusts the change in the prior beliefs given new information. Conservative learners ($\delta < 1$) under-weight the incoming observations and revise their prior beliefs less than expected from Bayes' theorem. Liberal learners ($\delta > 1$) overwrite their prior beliefs quicker than the optimal Bayesian learner, characterized by $\delta = 1$. Thus the learning rate governs the degree to which learners revise their prior beliefs about the probabilities of risky outcomes with a new outcome that they learn about.}

The learning process of the Bayesian (BVU) model is illustrated in Figure \ref{fig:sensi}\added[id=jj]{, showing the model's prior and posterior beliefs about the probability of a gain as a function of the sample size and the prior beliefs about the gain probability. The orange curves show different initial beliefs, the purple and black curve illustrate the change in belief after observing a small representative sample of outcomes (\textit{t}$=$5) and a large representative sample (\textit{t}$=$30, with equal relative frequencies in both samples). For large samples, the beliefs approximate the true probability with which the gain occurs for a gamble, independently of the prior belief. In contrast, for small samples, the prior belief changes the models' posterior belief about the gamble's gain probability (purple curve). Unlike the Bayesian model, the relative frequency model predicts a constant $Pr(\text{gain})$ probability equal to the true $Pr(\text{gain})$ probability after the model saw the representative sequence of outcomes, such as 1 gain outcome out of 5 outcomes or 6 gain outcomes out of 30 outcomes.}

\begin{figure}[htbp] 
  \centering
\includegraphics[width=1\linewidth, keepaspectratio]{fig1.eps}
  \caption{\added[id=jj]{\BVU model prediction about probability beliefs depending on different gambles, sample sizes, and prior belief distributions. Predicted beliefs are obtained using $\delta=1$. (a) Beliefs about a gamble with low probability of a gain. Shown are the distribution, mean, and quantile interval of the prior belief and the posterior beliefs after seeing samples of size 5 and 30 (small and large, resp); left, middle, and right panels show model behavior for three types of prior beliefs. (b) Beliefs about a gamble with high probability of a gain.}}
  \label{fig:sensi}
\end{figure}

The Bayesian (BVU) model predicts that people's prior beliefs and the sample size influence their (posterior) valuations. This is because as sample size increases, the beta-distributed belief shifts from the mean of the prior belief in the direction of the true underlying probability of the gain (see Figure \ref{fig:sensi}). Whether the model predicts a decrease or an increase in valuations with growing sample size depends on the shape of the prior belief and the true probability of a gain. In a situation with a true gain probability of less than 50\%, learners with a uniform prior belief or a prior beliefs that a gain is very likely should decrease their valuations as a function of sample size, according to the BVU model. Learners with a prior belief that a gain is very unlikely, however, should increase their valuations as a function of sample size. By contrast, if the true gain probability is larger than 50\%, learners with uniform priors should increase their valuations with sample size, and decrease them when having the prior belief that gains are very likely. In all cases, with increasing sample size the posterior beliefs becomes more and more accurate. Therefore, the model also predicts that confidence in valuations increases with increasing sample size. %We also test this prediction by assessing confidence ratings.

\section{Study 1}
In Study 1, we aimed to measure the preference strength for risky options, and therefore, we prompted participants to evaluate individual gambles \citep[similer to e.g.,][]{Ashby2014, Golan2014, Pachur2012}, because evaluations allow us to measure preference strength more precisely than binary choices between gambles. This is important, as detecting subtle differences in strength of preference \replaced{may require}{requires} fine-grained measurement scales. \added[id=jj]{Following \cite{Griffin1992}, we also asked participants for their confidence in their valuations to test if confidence were higher in small samples as found by \cite{Griffin1992}.} Second, we aimed to lay bare the precise effects of sample size---therefore, we presented each gamble with various sample sizes. \added[id=jj]{We presented representative samples, which eliminates sampling error and allows} for an unbiased comparison of sample size effects. The order of outcomes within a sampling sequence was random. A third, but sub-ordinate, goal was comparing valuations from experience with valuations from description. Therefore, we added a block where people made valuations from description. However, as our main goals involved studying how preferences change as a function of sample size, we refrained from manipulating the order of the experience and description blocks. All people made valuations first from experience and afterward in a separate block from description. 

In Study 1, we examined how people form valuations from experience when they know the possible outcomes before they start sampling. In a situation where the outcomes are known, learning is simplified because participants only need to learn the outcome probabilities from experience. 

\subsection{Method}
\subsubsection{Participants}
Forty people (31 women, 9 men) from the participant pool of the University of Basel between 18 and 40 years old ($M = 23.41$ years, $SD = 4.86$) participated in the study. Participants could choose between a show-up fee of 10 CHF (approximately \$11.10 at the time of the experiment) or course credit. Additionally, each participant received a bonus payment that depended on the outcome of a randomly selected trial ($M = \$6.38$, $SD = \$7.62$).

\subsubsection{Stimuli and Design}
The design was a within-subject design, participants evaluated two-outcome gambles from experience and from description. Six gambles with either a gain or a zero outcome were created by crossing two gamble types (\textit{\$-bet}, \textit{p-bet}) with three expected values (2.00, 3.20, 4.00), see Table \ref{table:Lotteries}. Three gambles offered a high gain with low probability (\$-bet), and three offered a small gain with high probability (p-bet, notation following \citealp{Lichtenstein1971}). In the experience-based valuation each gamble was crossed with four sample sizes: extra small, small, medium, large (xs, s, m, l), listed in Table \ref{table:Lotteries}. As shown in Table~ \ref{table:Lotteries} the extra small sample size (xs) consisted of a sample size of 5, 6, or 7 outcomes, which allowed us to represent rare events of 1/5, 1/6, and 1/7 (20\%,  17\%, 14\%, respectively). The small sample size equaled xs $\times$ 2, the medium sample size equaled xs $\times$ 3, and the large sample size equaled xs $\times$ 6. Participants saw each gamble with each of the four sample sizes three times in fully randomized order (each gamble was seen 12 times).

To avoid memory effects between trials, 18 filler trials were shown\footnote{Filler trials involved six gambles with outcomes identical to Table~\ref{table:Lotteries}, but probabilities changed to 75\% (\$-bets) and 25\% (p-bets). The filler gambles' sample sizes were 4, 8, and 16.} Filler trials were excluded from analyses, because they only aimed at minimizing memory-effects.


<<table1>>=
R1$tab1

@

After the experience block, participants saw each gamble three times in a description format. In this block, participants received a description of the gamble's outcome and the associated probability; probabilities were rounded to one decimal place (1/5 = 20\%, 1/6 = 16.7\%, 1/7 = 14.3\%). Again, presentation order of all trials was fully randomized.


\subsubsection{Procedure}
Participants read printed instructions and completed a questionnaire that ensured their understanding of the instructions; the experimental task was computerized. \added[id=jj]{Participants judged the value of the risky gambles (see Table~\ref{table:Lotteries}) and rated the confidence in their judgments. Gambles were judged one-by-one, in the experience-based format given different sample sizes, followed by the description format. In the experience format, before sampling, participants were informed about the outcomes but not the outcome probabilities of a gamble, which they learned by repeatedly sampling outcomes. The possible outcomes were represented by numbers in a circle (gains were in a black circle, zero-outcomes white) and stayed on screen throughout sampling. Pressing the space bar sampled an outcome; the realized outcome appeared as a black or white circle for 250 ms; sampling ended after a predefined sample size (listed in Table~\ref{table:Lotteries}). Participants were informed a priori about the pre-specification of the sample size. The sampled outcomes were representative for the gambles (for example, drawing a sample of size 5 from a gamble with $Pr(\text{gain})=20\%$ resulted in one gain outcome and four zero outcomes). After sampling, participants evaluated the gamble and rated their confidence, and proceeded to the next trial. Sample size changed from trial to trial.} The trial order and outcome sequence within a trial was randomized for individual participants. 

Confidence was measured on a 7-point Likert scale (from 0 = \textit{very unconfident} to 6 = \textit{very confident}). Valuations of gambles were measured as selling prices following the Becker--DeGroot--Marschak (BDM) method \citep{Becker1964}: \deleted[id=jj]{For each gamble}Participants stated their selling prices in terms of the lowest monetary amount for which they would give up the right to gamble. Prices could range from 0.00 to the current gain in CHF, with increments of 0.10 CHF. The BDM auction was incentivized as follows. The selling price that a participant entered was compared to a random value drawn from all possible selling prices in the trial. If this random number was larger than the stated selling price, the participant ``sold'' the gamble and received a monetary amount \replaced[id=jj]{that equaled}{equal to} the random number. If the random number was equal to or smaller than the selling price, the participant got to gamble. For the BDM, the optimal strategy is always to report the true selling price.\footnote{\label{logic.BDM}
To understand this logic, consider the following example: A person has a true selling price for a given gamble of \$3. If this person sets her selling price too low (e.g., \$2) and the randomly generated number is \$2.10, she sells her gamble for a lower price than her true selling price. If, however, she sets her selling price too high (e.g., \$5) and the randomly generated number is \$4, she keeps and plays her gamble, although she actually would have preferred to receive the \$4.
} \added[id=jj]{Detailed instructions as well as an example of the BDM auction were presented to the participants to explain the auction's mechanism. We used a short questionnaire to ensure that all participants had understood how the auction worked.}

\subsection{Results}
The analyses were conducted in statistical software R 4.0.2 (2020-06-22) \citep{R} using the BayesFactor package v0.9.12 \citep{BayesFactor} and the cognitivemodels package v0.0.10 \citep{Jarecki2020}; inferences were based on the Bayes factor ($BF\textsubscript{ij}$).\footnote{Bayes factors quantify how much more or less likely the data are under Model i ($\mathrm{M}\textsubscript{i}$) than Model j ($\mathrm{M}\textsubscript{j}$). A Bayes factor of 1 ($BF\textsubscript{ij} = 1$) means that the data do not discriminate between the two models $\mathrm{M}\textsubscript{i}$ and $\mathrm{M}\textsubscript{j}$. A Bayes factor of 5 (i.e., $BF\textsubscript{ij} = 5$) means that the data are 5 times more likely under $\mathrm{M}\textsubscript{i}$ than under $\mathrm{M}\textsubscript{j}$. A Bayes factor of $1/5$ ($BF\textsubscript{ij} = 1/5$) means that the data are 5 times more likely under $\mathrm{M}\textsubscript{j}$ (note that $BF\textsubscript{ji} = 1/BF\textsubscript{ij}$).} 
%
%\input{analyses/reports/results_study1}


\subsubsection{Evaluations of gambles and sample sizes}

Table \ref{table:means_study1} summarizes the evaluations of the gambles
by format (description vs.~experience) and sample size (xs, s, m, l).
For instance, across sample sizes xs, s, m, and l, the participants
evaluated gamble 4 with an average of \(2.80, 2.91, 2.80, 2.95, 3.08\)
(respectively). Across gambles, the evaluations were not influenced by
sample sizes; a statistical model comparison among analyses of variance
(ANOVA) confirmed this, an ANOVA of the valuations with the predictors
gamble type and gamble expected value (Model 0) outperformed models with
the additional predictors sample size (\(BF_{01} = 438\)) and with a
sample-size x gamble-type interaction (\(BF_{02} > 1000\), all models
had by-participant random effects). Although the \$-bets and p-bets had
the same expected values, the \$-bet gambles with their higher outcome magnitudes and lower probabilities were evaluated higher
(\(M=5.61, SD=3.86\)) than the p-bet gambles (\(M=2.76, SD=1.07\)) with their lower outcome magnitudes and higher probabilities,
\(BF > 1000\) in favor of an ANOVA that predicts valuations as a
function of gamble type over an ANOVA without the predictor gamble type
(both models include by-participant and by-expected-value random
effects).

The valuations in the experience condition were not driven by primacy or
recency: neither the outcomes that participants sampled in the first
half of the sampling phase nor those sampled during the second half of
the sampling phase predict the observed valuations. A linear model of
the evaluations with the predictor gamble type
(\(\mathrm{M}\textsubscript{0}\)) outperformed a model with the
additional predictor mean of the first half of samples
(\(BF\textsubscript{01} = 16.6\)) and a model with the additional
predictor mean of the second half of samples
(\(BF\textsubscript{02} = 6.7\)).

\textit{Description versus experience.}
<<table2, include = TRUE, results = "asis">>=

R1$tab2

@

Participants' valuations from description and experience differed for most of the gambles and sample sizes (Table \ref{tab:means_study1}, rightmost column). The \$-bets were evaluated higher based on experience ($M=5.78, SD=3.94$) compared to description ($M=4.93, SD=3.45$). By contrast p-bets were evaluated lower on the basis of experience ($M=2.74, SD=1.09$) compared to description ($M=2.86, SD=0.98$), $BF > 1000$ in favor of an ANOVA including a gamble-type x condition interaction over a model with only the main effects. Thus, we found a D--E gap that differs from the classic D--E gap observed in choice paradigms. In Study 1, participants valued gambles as if they overweighted rare events from description \textit{and} experience. This effect was even stronger when participants made valuations from experience.

At the aggregate level, sample size seems to not influence the
evaluations of gambles in our decision from experience paradigm, but the
subsequent cognitive modeling analyses will show a more nuanced picture.




\subsubsection{Cognitive modeling}

By modeling participant's valuations with the two models we proposed, it
is possible to examine the effect of sample size on value judgments. We
tested how good the \added[id=jj]{relative frequency} (RF) model and the
\added[id=jj]{Bayesian value updating} (BVU) model represent
participants' value judgments of gambles. To check the psychological
plausibility of both models we also checked if both models outperform a
baseline model, which predicts a constant evaluation equal to the mean
individual evaluation.

\textit{Modeling Procedure.} To estimate the models, the observed and
predicted evaluations were normalized to a common range between 0 and 1
(by division through the gain of the presented gamble). Maximum
likelihood was used to estimate the parameters of the models at the
participant level, assuming that observations follow a truncated normal
distribution around the model predictions (truncation between 0 and 1)
with a constant standard deviation (\(\sigma\)), that was estimated by
participant (\(0 < \sigma \leq 1\)).
\added[id=jj]{Therefore, the relative frequency model had two free parameters, namely $\sigma$ and the power utility exponent $\tau$ ($0 \leq \tau \leq 20$). The Bayesian value updating model had four free parameters, the prior belief parameter for gain outcomes $\alpha_0$ ($0 \leq \alpha_0 \leq 2$), the learning rate $\delta$ ($0 \leq \delta \leq 10$), $\tau$ and $\sigma$, with the prior on zero outcomes constrained to $\beta_0 = 2-\alpha_0$. The baseline model had two free parameters, the mean evaluation $\mu$ and $\sigma$. We estimated the parameters using an augmented Lagrange multiplier method \citep[Rsolnp package, version 1.16]{Ghalanos2015}. Models were compared based on evidence strength and \added[jj2]{AIC} weights from the Akaike information criterion (AIC) \cite[evidence in favor of a model compared to the best-fitting model by participant][]{Kass1995, Lewandowsky2011}. Higher weights indicate stronger evidence for a model.}

We will first outline the quantitative model fit, followed by the
qualitative model fit, and lastly analyze the effects of sample size
given the cognitive strategies.


\textit{Quantitative Model Fit.} The Bayesian value updating model described the majority of the participants best \added[id=jj2]{(\Sexpr{round(R1$winners["bvu"])} of 40; \Sexpr{round(prop.table(R1$winners)["bvu"]*100)}\%). The relative frequency model described \Sexpr{round(R1$winners["rf"])} participants best (\Sexpr{round(prop.table(R1$winners)["rf"]*100)}\%); the baseline model described \Sexpr{round(R1$winners["base"])} participants best. Figure \ref{fig:fig2} shows the evidence strength for the models by participant. The models' mean Bayesian information criterion across all participants equaled BIC\textsubscript{BVU}  \(=\Sexpr{R1$bic["bvu"]}\),
BIC\textsubscript{RF}   \(= \Sexpr{R1$bic["rf"]}\), and
BIC\textsubscript{BASE} \(= \Sexpr{R1$bic["base"]}\) and the mean Akaike Information criterion was
AIC\textsubscript{BVU} \(=\Sexpr{R1$aic["bvu"]}\),
AIC\textsubscript{RF}  \(= \Sexpr{R1$aic["rf"]}\), and 
AIC\textsubscript{BASE}\(= \Sexpr{R1$aic["base"]}\)
(lower values indicate better fit). Even when restricting the learning rate $\delta$ to 1 in the Bayesian model, the Bayesian model outperforms the relative frequency model.}\footnote{\added[id=jj2]{A constrained Bayesian value updating model with no variation in the learning rate ($\delta=1$) described 26 of 40 or 65\% best; and the relative frequency model described 13 participants best (32\%); the baseline model described 1 participants best.}}

\begin{figure}[htb] \centering
    \includegraphics[width=.8\linewidth]{../figures/fig2-1}
    \caption{Study 1: Evidence for the models for individual participants. \textit{RF}$=$ relative frequency model, \textit{BVU}$=$ Bayesian value updating model, \textit{BASE}$=$ Baseline model.}\label{fig:fig2}
\end{figure}

The estimated parameters of the best-fitting models are shown in Table
\ref{tab:study1_parameter}. The power utility exponent that captures the subjective values of outcomes (\(\tau\)) did
not differ substantially between the participants that were best described
by the Bayesian value updating (BVU) model (\(M_{\tau}= \Sexpr{R1$par$bvu["tau"]}\)) and those best described by a relative frequency model (\(M_{\tau}=\Sexpr{R1$par$rf["tau"]}\)) (t-test of
\(\Delta\) \Sexpr{R1$par$BF}). The prior beliefs about the
probability of gains was $\Pr(\text{gain})=$\(\Sexpr{ R1$par$bvu["count_x"]/ sum(R1$par$bvu[c("count_x", "count_0")])}\) (resulting from estimated prior
parameters \(\alpha_0 = \Sexpr{R1$par$bvu["count_0"]}\), \(\beta_0 = \Sexpr{R1$par$bvu["count_x"]}\)) for the participants
best described by the BVU model. The learning rate of the participants described by the BVU model was, on average,
\(M_{\delta}=\Sexpr{R1$par$bvu["delta"]}\) (a model with values \(>\) 1 revises its priors faster than optimal Bayesian, 1 is optimal Bayesian, \(<\) 1 is
conservative compared to optimal Bayesian). The liberal learning rate is
unusual given that previous work has found conservative learning
\citep{Edwards1967,Tauber2017}. This fast learning in our task
\added[id=jj2]{could be explained} by that participants repeatedly
sampled from the same set of gambles.

<<table_par>>=
R1$tab_pars

@

\textit{Qualitative Model Fit.} The qualitative fit between the models
and the data is shown in Figure \ref{fig:fig3}, which plots the
predictions of the best-fitting models against the observed evaluations
by participant. The models generally describe the data well
(\added[id=jj2]{mean Pearson's} \(r\textsubscript{pred,obs} = \Sexpr{R1$qual_cor}\)),
except in four cases, where even the winning model fails to resemble the
data qualitatively (participants \Sexpr{R1$qual_no_fit}, with
\(r\textsubscript{pred,obs} < 0.40\)).\footnote{As robustness check we repeated the model comparison with subjective probability weighting, using Prelecâ€™s \citeyear{Prelec1998} single parameter weighting function. This weighting function incorporates non-linearities in the perception of probabilities. The quantitative results of the probability weighting model and the utility model favored a utility model without probability weighting. Thus, the results suggest that in the paradigm that we studied people's perception of the probabilities of outcomes are relative accurate and best described by a linear probability function.}

\begin{figure}[htb]

{\centering \includegraphics[width=\textwidth]{../figures/fig3-1} 

}

\caption{Study 1: Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). Darker color indicates overlapping points. \textit{BVU}$=$ Bayesian value updating model, \textit{RF}$=$ relative frequency model, \textit{BASE}$=$baseline model.}\label{fig:fig3}
\end{figure}

\added[id=jj]{
The cognitive modeling results thus show that most participants were best described by a Bayesian strategy, and a minority by a relative-frequency strategy. This strategy heterogeneity helps understanding the behavioral null finding---that sample size seemed to have no effect on valuations---that were observed at the aggregate level (Table \ref{tab:means_study1}). The statistical aggregate analysis fails to take the individual differences in learning strategies into account, while participants are best described by a mixture of strategies. Moreover, the aggregate statistical analysis without the cognitive modeling also fails to account for differences in the prior beliefs about gain probabilities.
}

\clearpage
%
\subsection{Discussion of Study 1}

Study 1 shows that the size of the sample overall did not have an effect on valuations from experience. Cognitive modeling revealed that the majority of participants were best described by a model following Bayesian principles which treats sample size as a measure of uncertainty \added[id=jj2]{\citep{Hoffart2019}}. However, a minority of participants were best modelled with a relative frequency strategy that disregards sample size. Valuations also differed when made from description versus experience. Interestingly, this difference is the opposite of the classic D--E gap that has been reported in choice paradigms: We found evidence that people behaved as if they overweighted rare events when making valuations from description \textit{and} experience. Notably, this overweighting of rare events was even stronger for the experience condition. Importantly, people assigned higher valuations to \$-bets (i.e., gambles that provide a high reward with low probability) than p-bets (i.e., gambles that provide a small reward with low probability) even although both gambles had similar expected values. This finding is in line with recent research that suggested people overweight extreme outcomes in experience-based tasks \citep{Ludvig2017} \added[jj2]{
and with research suggesting that when participants need not focus on learning the outcome values but learn only the probabilities during sampling, people tend to overweight rare events \citep{Hotaling2019}}.

\section{Study 2}
In Study 1 participants were informed about the possible outcomes before drawing outcomes, which differs from typical decisions from experience paradigms, in which people learn about the outcome values by sampling. To bring our method more in line with the standard decision from experience paradigm, experimental method was adapted for Study~2 such that participants learned the magnitude of one outcome during sampling.

\subsection{Methods}
The procedure an methods of Study~2 were similar to Study~1. The only difference consisted in that before participants started sampling, they were now only informed about gambles being two-outcome gambles and that one of the outcomes was zero. Unlike in Study 1, the gain amounts of the gambles were not displayed on screen before or during the sampling stage.

\subsection{Participants}
Forty people (38 women, 2 men) between 18 and 27 years old ($M = 21.1$ years, $SD = 2.05$), recruited from the subject pool of the University of Basel, participated. We followed the incentive scheme of Study 1. On average, participants earned a bonus of 7.16 CHF ($SD = 6.91$ at the time of the study). For the data analysis, we compared the same statistical models as described in Study 1.

\subsection{Results}
As before, the analyses were conducted in the software R, models were estimated as in Study 1.


\subsection{Evaluations of Gambles by Condition and Sample Size}

Table \ref{table:study2means} displays participants' mean and median
evaluations per gamble in the experience and description conditions by
sample-size. The results resemble the findings of Study 1, namely that
larger sample sizes do not lead to systematic changes in mean
evaluations. Evaluations were best predicted by a linear model including
expected value as random factor and gamble type as fixed factor gamble
(additional fixed factor sample size: \(BF\textsubscript{01} = 285\);
additional interaction between sample size and gamble type:
\(BF\textsubscript{01} > 1,000\)).

<<study2means>>=
R2$tab1

@

\subsubsection{Modeling Results}

The next section presents the quantitative cognitive model fit, the qualitative model fit, and the effects of sample size conditional on the models that describe participants.

\textit{Quantitative Model Fit.} The Bayesian value updating model
described the majority of the participants best (23 of 40; 57\%). The
relative frequency model described 17 participants best (42\%); the
baseline model described no participants best. Figure
\ref{fig:study2_model_weights} shows the evidence strength for the
models by participant. The models' mean Bayesian information criterion
across all participants equaled BIC\textsubscript{BVU}\(= -96\),
BIC\textsubscript{RF}\(= -91\), and BIC\textsubscript{BASE}\(= -17\)
(lower values indicate better fit).

\begin{figure}[htb]

{\centering \includegraphics{../figures/study2_model_weights-1} 

}

\caption{Evidence for the models for individual participants. \textit{RF}$=$ relative frequency model, \textit{BVU}$=$ Bayesian value updating model, \textit{BASE}$=$ Baseline model.}\label{fig:study2_model_weights}
\end{figure}

\added[id=jj]{
The estimated parameter of the winning models are shown in Table \ref{tab:study2_parameter}. The mean power utility exponent $\alpha$ is relatively similar for participants classified as using a Bayesian value updating strategy ($M_{\alpha}= 2.19$) compared to those using a relative frequency strategy ($M_{\alpha}=2.15$), $M = 0.04$ 95\% HDI $[-1.08$, $1.16]$, $\mathrm{BF}_{\textrm{01}} = 3.21$. Like in Study 1, the participants using the Bayesian strategy had a loss prior, on avarage; they believed that gains occur with 45\% (gain prior $\theta_G = 0.91$; zero-outcome prior $\theta_0 = 1.09$). Again, the learning rate parameter $\delta$ shows liberal learning ($M_{\delta}=2.27$).
}

<<table_par_study2>>=
R2$tab_par

@

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:study2_parameter}Parameter Estimates of Winning Models, \textit{M (SD)}}

\begin{tabular}{lcccc}
\toprule
Winning Model & $\alpha$ & $\delta$ & $\theta_G$ & $\sigma$\\
\midrule
BVU (\textit{n}$=$23) & 2.19 (2.35) & 2.27 (3.39) & 0.91 (0.79) & 0.23 (0.22)\\
RF (\textit{n}$=$17) & 2.15 (1.45) & -- & -- & 0.15 (0.04)\\
\bottomrule
\addlinespace
\end{tabular}

\begin{tablenotes}[para]
\normalsize{\textit{Note.} \textit{BVU}$=$ Bayesian value updating model, \textit{RF}$=$ relative frequency model. Parameters denote: $\alpha=$ power utility exponent, $\theta_G$ gain prior, $\sigma$ standard deviation.}
\end{tablenotes}

\end{threeparttable}
\end{center}

\end{table}

\textit{Qualitative Model Fit.}
\added[id=jj]{Figure \ref{fig:ind_fits1} illustrates the qualitative model fit by plotting the predictions of the best-fitting models against the observed evaluations. The models generally describe the data well (mean $r\textsubscript{pred,obs} = 0.70$). However, some participants data are qualitatively not well described by the winning models (participants number 02, 07, 14, 27, with $r\textsubscript{pred,obs} < 0.40$). For one of these cases, the relative frequency model was favored, for the remaining three cases the Bayesian value updating model was favored.\footnote{Note, however, that participant number 7 evaluated all gambles with zero, which indicates inattentiveness or not comprehending the task.}}

\begin{figure}[htb]

{\centering \includegraphics{../figures/ind_fits1-1} 

}

\caption{Predicted evaluations from the best-fitting models plotted against the observed evaluations (by participant). \textit{BVU}$=$ Bayesian value updating model, \textit{RF}$=$ relative frequency model, \textit{BASE}$=$baseline model.}\label{fig:ind_fits1}
\end{figure}

\added[id=jj]{The cognitive modeling results thus show greater heterogeneity in Study 2 (compared to Study 1), slightly more than half the participants could be described by a Bayesian strategy, the remaining participants followed a relative-frequency strategy. The higher prevalence of the relative frequency strategy in Study 2 can be explained considering that Study 2 initially withheld the information about the magnitude of the gain outcomes as opposed to Study 1, where participants received outcome information. If participants learn not only probabilities but also outcomes, the task might require more cognitive effort and this might take attention away from prior beliefs.}

\emph{The effect of sample size given cognitive strategies.}
\added[id=jj]{We qualitatively investigated how sample size affects the evaluations of relative-frequency-type and Bayesian-type learners. The Bayesian model predicts that sample size changes the evaluations in interaction with the prior beliefs. Participants with a gain prior should decrease the evaluations of \$-bets as sample sizes increase; participants with a zero-outcome prior should increase their evaluations of p-bets as sample size increases.
}

\added[id=jj]{
Given the cognitive modeling results, participants were classified as relative-frequency learners, Bayesian learners with gain priors, and Bayesian learners with loss priors (prior parameters $\theta_G > 1$, and $\theta_G \leq 1$, respectively). Figure \ref{fig:qual2} shows how sample size affects the three learner types' evaluations of p-bets and \$-bets. For \$-bets the evaluation of the Bayesian learners show a small interaction with the prior. Statistical analyses by means of a Bayesian generalized linear model\footnote{regressing the (normalized) evaluations on the predictors sample size, gamble type (p-bet, \$-bet), and type (BVU-gain-prior, BVU-loss-prior, RF) with a by-participant random intercept; categorical predictors were effects-coded to facilitate interpretation of interactions \citep[for details, see][]{SingmannForthcoming}}, however, showed no substantial support that including the learner type as predictor (model M\textsubscript{0}) improves goodness of fit compared to excluding learner type as predictor (model M\textsubscript{0}), BF\textsubscript{01} $=0.00$. These results corroborate the findings from Study 1.
}

\begin{figure}[htb]

{\centering \includegraphics{../figures/qual2-1} 

}

\caption{Mean evaluation (standardized to 0 - 1) by winning model and prior beliefs of the BVU model. \textit{BVU}$=$Bayesian value updating model, \textit{RF}$=$ Relative frequency model. Error bars indicate standard errors. \textit{\$-bet}: low-probability high-outcome gambles, \textit{p-bet}: high-probability low-outcome gambles. Sample sizes (xs, x, m, l), see Table \ref{tab:Lotteries}. \textit{n=16, 13, 6} denotes the number of participants best-described by the respective models.}\label{fig:qual2}
\end{figure}

\subsubsection{Effect of gamble type and sampling order}

In line with Study 1, the mean and median ratings in Table
\ref{tab:means_study2} show that \$-bets are evaluated higher than
p-bets, although he gamble types had the same expected value
(BF\textsubscript{10} \textgreater 1,000). Again, we did not find
evidence for any recency or primacy effects.
\(\mathrm{M}\textsubscript{0}\), which predicts valuations as a function
of the factor gamble (1--6), was slightly preferred over models that
also include the mean of the first half of the observed samples
(\(\mathrm{M}\textsubscript{1}\)) or the mean of the second half of the
observed samples (\(\mathrm{M}\textsubscript{2}\)) as predictors
(\(BF\textsubscript{01} = 3.4\), \(BF\textsubscript{02} = 1.9\)).

\subsubsection{Confidence ratings}

Participants' mean confidence ratings of their valuations from
experience in the extra small, small, medium, and large sample-size
categories were (\(M (SD)\)) equal to 4.01 (\(1.23\)), 4.09, (\(1.14\)),
4.04 (\(1.19)\), and 4.16 (\(1.19\)). for sample size categories xs, s,
m, and l, respectively. Sample size did not influence participants'
confidence systematically: \(\mathrm{M}\textsubscript{0}\), which
predicts confidence rating as a function of a random participant effect,
was strongly preferred over \(\mathrm{M}\textsubscript{1}\), which in
addition includes the sample-size category as a predictor
(\(BF\textsubscript{01} = 737\)).

\added[id=jj]{The Bayesian cognitive strategy predicts that confidence increases with higher sample sizes; the relative frequency model predicts that sample size has no influence on confidence. To test this, the confidence data was analyzed separately by best-fitting cognitive model (Bayesian-type learners or relative-frequency-type learners). Contrary to the expectations, the Bayesian learners' confidence did not increase with higher sample sizes ($M=3.94, 4.01, 3.96, 4.12$ for sample sizes xs, s, m, l, respectively. Respective \textit{SD}s $= 1.29, 1.18, 1.20, 1.25$). A linear model\footnote{with by-participant random intercept and the predictors effect-coded.} of the confidence ratings, which excluded sample size as predictor, was preferred over a model including sample size (BF\textsubscript{excl,incl} $= 100$). Similarly, the confidence of relative-frequency-type learners did not show an effect of sample size on confidence ratings ($M=4.11, 4.20, 4.14, 4.21$ for sample sizes xs, s, m, l, respectively; \textit{SD}s $= 1.12, 1.08, 1.17, 1.10$, respectively; BF\textsubscript{excl,incl} $= 0.70$)}.

\subsubsection{Description versus experience}

The above Table \ref{tab:means_study2} summarizes the valuations from
description; it also shows the difference between the mean valuations in
the experience and description conditions (column D--E, separately for
different sample sizes). Further, it provides the Bayes factors,
quantifying the evidence in favor of a difference between valuations
from experience and those from description.

Valuations made from description and experience differed for most of the
gambles and sample sizes (see Table \ref{tab:means_study2}, rightmost
column). In particular, participants attached a higher value to
experienced than to described \$-bets (Gambles 1--3) but attached a
higher value to described than to experienced p-bets (Gambles 4--6).
Thus, we found a D--E gap that is the opposite of the classic D--E gap
observed in choice paradigms. In our study, participants valued gambles
as if they overweighted rare events from description \textit{and}
experience. This effect was even stronger when people made valuations
from experience.

We also compared participants' confidence ratings of valuations from
experience in each sample-size category to those from description
(\(M = 4.04\), \(SD = 1.08\)). Separately for each sample-size category,
we compared \(\mathrm{M}\textsubscript{0}\), which predicts confidence
as a function of random participant effects, with
\(\mathrm{M}\textsubscript{1}\), which takes condition as an additional
fixed factor into account. The analyses suggest that participants were
slightly more confident about their ratings from experience than from
description for small (\(BF\textsubscript{10} = 2.5\)), medium
(\(BF\textsubscript{10} = 1.3\)), and large
(\(BF\textsubscript{10} = 4.8\)) sample sizes. For the extra small
sample sizes (\(BF\textsubscript{10} = 0.3\)), confidence judgments did
not differ.
\clearpage

\subsection{Discussion of Study 2}
Study 2 tested whether the results of Study 1  generalize to typical sampling paradigms where people learn the outcome values from experience. The results of Studies 1 and 2 are very similar: participants mean evaluations were not influenced by sample size systematically, and in line with Study 1, we found that a substantial proportion of people could be classified as Bayesian learners. In Study 2, however, a greater proportion of participants was described by a \added[id=jj]{potentially cognitively simpler} relative-frequency-based cognitive model.

\clearpage %forces figures and tables to be printed HERE

\section{Test of Qualitative Predictions from the Cognitive Models}
\added[id=jj]{The next analyses examine qualitative predictions from the Bayesian value updating and the relative-frequency model of learning probabilities. We pooled the observed data from both studies (N= 80) and classified participants by the best-fitting models from the cognitive modeling results.}


\textit{Predictions about gamble valuations.}
\added[id=jj]{Regarding the valuations, the relative frequency model predicts no effect of sample size on valuations in our task (because the relative frequencies did not differ across sample sizes). In contrast, the Bayesian value updating model predicts that given different prior beliefs the frequency of experienced outcomes will change people's beliefs in different directions, and that this change is stronger with larger sample size (Figure~ \ref{fig:sensi}). According to the Bayesian value updating model, participants with a prior belief that zero outcomes are more likely than gain outcomes (zero-outcome prior) who sample p-bets should increase their evaluations with higher sample size. By contrast, participants with a prior belief that gains are more likely than zero outcomes (gain prior) who sample \$-bets should decrease their valuations with higher sample size.}

\input{analyses/reports/results_evaluation-by-prior}

\textit{Discussion of the qualitative cognitive model predictions.}
\added[id=jj]{Learners that seemed to follow a Bayesian strategy changed their evaluations in line with Bayesian principles, but also the learners that followed a relative frequency strategy showed slight changes in evaluations with sample size. Regarding the influence of sample size on confidence ratings, the Bayesian model's prediction of growing confidence with growing sample size received no support.}

\section{General Discussion}
A number of studies have shown that the amount of experience people gain with the outcome distribution of risky options affects risk preferences and risky choice. We manipulated the amount of experience (sample size) with risky options to investigate specifically, how the sample size influences risk preferences and evaluations in the gain domain.

Study 1 consisted of an experience-based sampling task in which participants were informed about the outcomes of \added[id=jj2]{binary} risky options. By sampling outcomes participants learned about the probabilities with which outcomes occurred. To examine the effect of experience on people's evaluation of risky gambles we manipulated the sample size of outcomes participants drew within-subject. The samples were predetermined so that the relative frequencies of the samples were representative of the expected value, which allowed us to control for sampling errors and isolate the mere effects of sample size on judgments. We tested whether participants' evaluation could be best predicted by a relative frequency (RF) model, that essentially ignores sample size, or a Bayesian value updating (BVU) model, that takes sample size into account. Most participants' evaluations were best described by the BVU model with an initial prior belief that the zero outcome occured with a larger probability than the positive gain outcome. These prior belief were however quickly overcome by learning from experience. Only around a quarter of participants were best-described by the RF model. Participants' confidence in evaluations, however, were not substantially affected by sample sizes.

\added[id=jj]{Study 2 was quite similar to Study 1, with the only difference that the potential outcomes of the risky options were not told to the participants before sampling outcomes. Therefore the potential outcomes had to be learned during the sampling process. Again, the size of the samples that participants drew was manipulated. The results are in line with Study 1: while mean evaluations and confidence were largely unaffected by the sample size manipulation, cognitive modeling results showed that a Bayesian strategy described the behavior of slightly more than half of participants well (57\%) and that the learners had an initial belief that the zero outcome was more likely than the positive outcome, which they updated fast through sampling.}

\added[id=jj]{The relative frequency model described a minority of participants best in both studies. This finding is in line with previous results suggesting that people often believe in the representativeness of short sequences of outcomes \citep{Griffin1992, Tversky1971}. This could also explain why people in experience-based tasks often do not sample much: If people believe that a short sequence of outcomes represents a prospect's outcome distribution comprehensively, they do not need to sample much, as they believe that sampling more will not yield new information.}

Our results suggests a substantial degree of heterogeneity of how people sample information and integrate the observed information for a final judgment. It is an open question why some of our participants behaved according to Bayesian principles and others did not. Thus, future research is needed to examining what factors influence whether people behave according to Bayesian principles. One possible candidate that mediates what strategy people use could be numerical literacy, which has been shown in previous work to be related to performance in Bayesian reasoning tasks \citep{Brase2017}.

\subsubsection{The role of prior beliefs}
\added[id=jj]{The Bayesian model assumes that people start sampling with an initial belief about the probabilities of different outcomes. Our data suggest that most Bayesian learners started with the prior belief that the zero-outcome was quite likely, but this belief was then quickly overruled by incoming experience. In contrast to previous work, which has indicated that people are slow Bayesian updaters, we find that people update quicker than optimal Bayesian with a liberal learning rate.}

We have assumed that over the course of the experiment and across different gambles, people's prior beliefs do not change. However, recent research suggests that knowing the gain amount elicits a more informed prior belief about the probability. Under uncertainty, people's beliefs about gain probabilities are informed by the gain amount: Large gains are believed to be less likely than small gains when no other information about a gamble than the gain amount is given \citep{Pleskac2014, Hoffart2018}. If in our studies people indeed used gain amounts as predictors of the probability and integrated this information with their samples, this can explain why we see only a small effect of sample size on gamble valuations. This is because in our experimental design large gains were indeed less likely than small gains. Thus, it corresponded to the probability--reward pattern that people might presuppose. If people start sampling with prior probability beliefs that correspond closely to the real outcome probabilities, new incoming information does not shift probability beliefs as dramatically as when the prior beliefs are uninformed (e.g., uniform priors). Future research could investigate this hypothesis, for instance, by investigation how sample size influences valuations in \textit{nonrepresentative} environments, such as environments where the size of a reward/gain is uncorrelated with the probability that it occurs.

\subsection{Description versus experience in choice and valuation}
\added[id=jj]{Study 1's findings differ from the classic description-experience gap, that is the  participants did not behave as if they overweight rare events from description and underweight rare events from experience. In the valuation tasks participants behaved as if they overweighted rare events from both description and experience, when they were informed about the outcomes before sampling.}
Importantly, in contrast to standard experience-based choice tasks where participants experience two lotteries and make binary choices, the participants had to evaluate one individual gamble in each trial. \cite{Lichtenstein1971}, for instance, report preference reversals when people evaluate rather than chose options (a person who chooses Gamble A over Gamble B may assign a higher value to Gamble B than to Gamble A). Our valuation data is not in line with other work on evaluations from experience, that finds stronger over-weighting of rare events in an experience- than a description-format \citep{Golan2014}, but this design differs from our task in that it did not use representative outcome sequences. As we argued in the Introduction, in free-sampling paradigms, the experienced relative frequencies of outcomes do not necessarily match the underlying probability and the resulting sampling error might have contributed to the difference in the results. It is further conceivable that people show different information processing in free- and forced-sampling paradigms. For instance, people may pay more attention to a voluntarily sampled outcome than to a sampled outcome that resulted from a forced sampling decision.

\subsubsection{Buying versus selling prices}
In both studies participants were asked for their selling prices of the gambles to elicit people's preferences. The endowment effect describes that people attach higher value to goods when they sell them than when they buy them \citep{Thaler1980}. \cite{Pachur2012} demonstrated this endowment effect in an experience-based task. It would be interesting to replicate our experiments with buying instead of selling prices and more rigorously compare the results with buying and selling prices from description. However, this was beyond the scope of the present study. Here, our main focus was to investigate whether preferences are influenced by sample sizes. This question can be answered independently of response format. Irrespective of whether people are asked for selling prices or buying prices or make repeated choices, the Bayesian model predicts an effect of sample size on valuations. 

\subsubsection{Confidence in valuations from description and experience}
People were more confident with their gamble valuations from experience than from description. This finding is remarkable given that from a normative perspective one could argue that people feel more confident when making valuations from description than when making valuations from experience. This is because valuations from description do not entail any uncertainty about the outcome distributions, whereas valuations from experience may entail such uncertainty.

Our findings do support research by \cite{Bradbury2014}. They reported that people who made investment decisions in the laboratory felt better informed and were more confident about their decisions from experience than those from description.
Potentially, people perceive and process experienced information differently from described information \citep{Kahneman2009}: 
While information that is described is more abstract, experienced information has direct salience or impact. When people draw a sample, they not only observe the outcome but also experience emotional reactions resulting from that observation. Thus, people do not only learn plain facts about the outcome distribution; they also gain a more vivid understanding of this distribution. This more concrete understanding of the risky option \added[id=jj2]{might} help people access their preferences and thus creates higher confidence in valuations.

\subsubsection{Summary}
\added[id=jj2]{
Our findings have shown that when the probabilities of risky options have to be learned from experience, sample size has no systematic effect on people's evaluations of the risky options when learners experience a representative sequence of outcomes (no sampling error). In this environment, the learning of the expected value of individual risky options mostly follows Bayesian updating principles with a prior belief that is slightly pessimistic (prior belief that the low outcome is more likely than the high outcome) but that is overwritten quickly--in fact quicker than optimal Bayesian updating--by the incoming evidence about the true probabilities of the risky option.} 


\bibliography{references}

\end{document}
